[
  {
    "title": "🚀 Built a Tool to Help You Speak Under Pressure—Feedback Wanted!",
    "content": "Practice speaking under pressure with SpeakFast.ai—improve clarity, confidence, and emotional delivery in real-world scenarios.\n\nHey folks,\nI’ve been hacking on this tool for a couple of weeks. It’s not perfect but it's ready for public feedback.\n🚀 Why I Built This\nBeing able to think and articulate clearly under pressure is a skill—one that most of us never practice, even though it shapes how we’re perceived in \"high-stakes\" conversations.\nWe’ve all had those moments:\n\nYou're put on the spot, and your mind blanks.\nYou have a great idea but can’t explain it fast enough.\nYou try to advocate for yourself, but the words don’t come out right.\n\nIf that sounds familiar, this might be useful to you.\n🎙 What I Built\nSpeakFast.ai is a tool to help you practice thinking and speaking under pressure through AI-driven simulated conversations.\n\nPick a preset scene (e.g., job interviews, salary negotiations, investor pitches).\nStart a live conversation with AI.\nGet instant feedback on clarity, confidence, and perception.\n\nI’m giving away 30 minutes of free access—just go to SpeakFast.ai and start a session.\n🤔 Why Not Just Use ChatGPT?  (or other similar tools)\nI love ChatGPT’s voice mode and it's possible to prompt it to do something like this, but it’s missing a few things:\n\nNo feedback on tone, confidence, or clarity.\nToo easy to mislead—it only focuses on what you say, not how you say it.\nNot designed for real-world pressure—it answers questions, gives solid advice, but it wasn't designed for this niche use case.\n\nI've tried other tools as well, but I did not find one I enjoyed spending time on.\n🔍 What I Need From You\nI’m still figuring out exactly who this is for and what you need. Early feedback from family and friends shaped the initial experience, but I’m open to major changes.\nIf this sounds remotely useful to you, I’d love for you to try it and tell me what’s missing.\n\nPS: Product is still early, so expect some bugs. Let me know if anything breaks!\n\n👉 Try SpeakFast.ai now!\nWould appreciate any thoughts—positive or critical. Thanks!",
    "url": "https://www.indiehackers.com/post/built-a-tool-to-help-you-speak-under-pressure-feedback-wanted-12e9422c44",
    "crawl_time": "2025-02-10 06:42:08",
    "translated_content": "练习在Speakfast.ai下在压力下说话 - 在现实世界中的清晰度，信心和情感交付。\n\n嘿，伙计们，\n我已经在这个工具上黑客了几周。它并不完美，但已准备好获得公众反馈。\n🚀为什么我建造这个\n能够在压力下清楚地思考和清楚地表达的是一项技能，即使我们大多数人都从未练习过，即使它塑造了我们在“高风险”对话中的看法。\n我们都有那些时刻：\n\n您当场，头脑空白。\n您有一个好主意，但不能足够快地解释它。\n您尝试自己提倡，但话语不是正确的。\n\n如果听起来很熟悉，这对您可能很有用。\n🎙我建造的\nSpeakFast.AI是一种工具，可以通过AI驱动的模拟对话来帮助您在压力下进行思考和讲话。\n\n选择一个预设场景（例如，求职面试，工资谈判，投资者的推销）。\n与AI进行现场对话。\n获得有关清晰度，信心和感知的即时反馈。\n\n我将提供30分钟的免费访问权限 - 只需访问Speakfast.ai并开始会议。\n🤔为什么不使用chatgpt？  （或其他类似工具）\n我喜欢Chatgpt的语音模式，可以提示它做类似的事情，但是它缺少一些事情：\n\n没有关于语气，自信或清晰度的反馈。\n太容易误导了 - 它只专注于您所说的话，而不是您说的话。\n不是为现实世界的压力而设计的，它回答问题，提供了可靠的建议，但不是为这种利基用例而设计的。\n\n我也尝试过其他工具，但是我没有找到我喜欢花时间的工具。\n🔍我需要你的东西\n我仍在确切地弄清楚这是谁以及您需要什么。家人和朋友的早期反馈塑造了最初的经历，但我愿意进行重大变化。\n如果这听起来对您有用，我希望您能尝试一下，并告诉我缺少什么。\n\nPS：产品还很早，因此可以期待一些错误。让我知道是否有任何破裂！\n\n👉现在尝试使用Speakfast.ai！\n会欣赏任何想法 - 阳性或关键。谢谢！",
    "category": "产品"
  },
  {
    "title": "Should I launch the landing page on product hunt and not a working MVP?",
    "content": "Hi guys,\nI have the landing page (which looks beautiful) with a lead form and waiting list but not a working product. Should I launch it like that and hope for waiting list enrollments and feedback on the idea or wait to finish the MVP and launch then? Or both? Thanks a lot!",
    "url": "https://www.indiehackers.com/post/should-i-launch-the-landing-page-on-product-hunt-and-not-a-working-mvp-6bc3fd6973",
    "crawl_time": "2025-02-10 06:42:14",
    "translated_content": "Hi guys,\nI have the landing page (which looks beautiful) with a lead form and waiting list but not a working product. Should I launch it like that and hope for waiting list enrollments and feedback on the idea or wait to finish the MVP and launch then? Or both? Thanks a lot!",
    "category": "其他"
  },
  {
    "title": "Data Analysis Showdown: Comparing SQL, Python, and esProc SPL",
    "content": "Talk is cheap; let’s show the codes.\n1. User Session Count\nUser behavior data table\n\nA session is considered over if a user does not take any action within 10 minutes, or if they do not log in within 5 minutes after logging out. Calculate the number of sessions for each user.\nSPL\n\nSQL\nWITH login_data AS (\n    SELECT userid, action_type, action_time,\n        LAG(action_time) OVER (PARTITION BY userid ORDER BY action_time) AS prev_time,\n        LAG(action_type) OVER (PARTITION BY userid ORDER BY action_time) AS prev_action\n    FROM session_data)\nSELECT userid, COUNT(*) AS session_count\nFROM (\n    SELECT userid, action_type, action_time, prev_time, prev_action,\n        CASE\n            WHEN prev_time IS NULL OR (action_time - prev_time) > 60 \n                OR (prev_action = 'exit' AND (action_time - prev_time) > 300 )\n            THEN 1\n            ELSE 0\n        END AS is_new_session\n    FROM login_data)\nWHERE is_new_session = 1\nGROUP BY userid;\n\nPython\nlogin_data = pd.read_csv(\"session_data.csv\")\nlogin_data['action_time'] = pd.to_datetime(login_data['action_time'])\ngrouped = login_data.groupby(\"userid\")\nsession_count = {}\nfor uid, sub_df in grouped:\n    session_count[uid] = 0\n    start_index = 0\n    for i in range(1, len(sub_df)):\n        current = sub_df.iloc[i]\n        last = sub_df.iloc[start_index]\n        last_action = last['action_type']\n        if (current[\"action_time\"] - last[\"action_time\"]).seconds > 600 or \\\n            (last_action==\"exit\" and (current[\"action_time\"] - last[\"action_time\"]).seconds > 300):\n            session_count[uid] += 1\n        start_index = i\n    session_count[uid] += 1\nsession_cnt = pd.DataFrame(list(session_count.items()), columns=['UID', 'session_count'])\n\n2. Count the players who score 3 times in a row within 1 minute\nScore table of a ball game\n\nSPL\n\nSQL\nWITH numbered_scores AS (\n    SELECT team, player, play_time, score,\n        ROW_NUMBER() OVER (ORDER BY play_time) AS rn\n    FROM ball_game)\nSELECT DISTINCT s1.player\nFROM numbered_scores s1\n    JOIN numbered_scores s2 ON s1.player = s2.player AND s1.rn = s2.rn - 1\n    JOIN numbered_scores s3 ON s1.player = s3.player AND s1.rn = s3.rn - 2\nWHERE (s3.play_time - s1.play_time) <60 ;\n\nPython\ndf = pd.read_csv(\"ball_game.csv\")\ndf[\"play_time\"] = pd.to_datetime(df[\"play_time\"])\nresult_players = []\nplayer = None\nstart_index = 0\nconsecutive_scores = 0\nfor i in range(len(df)-2):\n    current = df.iloc[i]\n    if player != current[\"player\"]:\n        player = current[\"player\"]\n        consecutive_scores = 1\n    else:\n        consecutive_scores += 1\n    last2 = df.iloc[i-2] if i >=2 else None\n    if consecutive_scores >= 3 and (current['play_time'] - last2['play_time']).seconds < 60:\n        result_players.append(player)\nresult_players = list(set(result_players))\n\n3. Calculate the number of users who are active for three consecutive days within every 7 days\nUser login data table\n\nSPL\n\nSQL\nWITH all_dates AS (\n    SELECT DISTINCT TRUNC(ts) AS login_date\n    FROM login_data),\nuser_login_counts AS (\n    SELECT userid, TRUNC(ts) AS login_date, \n        (CASE WHEN COUNT(*)>=1 THEN 1 ELSE 0 END) AS login_count\n    FROM login_data\n    GROUP BY userid, TRUNC(ts)),\nwhether_login AS (\n    SELECT u.userid, ad.login_date, NVL(ulc.login_count, 0) AS login_count\n    FROM all_dates ad\n    CROSS JOIN (\n        SELECT DISTINCT userid\n        FROM login_data) u\n    LEFT JOIN user_login_counts ulc\n    ON u.userid = ulc.userid\n    AND ad.login_date = ulc.login_date\n    ORDER BY u.userid, ad.login_date),\nwhether_login_rn AS (\n    SELECT userid,login_date,login_count,ROWNUM AS rn \n    FROM whether_login),\nwhether_eq AS(\n    SELECT userid,login_date,login_count,rn,\n        (CASE \n            WHEN LAG(login_count,1) OVER (ORDER BY rn)= login_count \n                AND login_count =1 AND LAG(userid,1) OVER (ORDER BY rn)=userid \n            THEN 0 \n            ELSE 1 \n        END) AS wether_e \n    FROM whether_login_rn\n),\nnumbered_sequence AS (\n    SELECT userid,login_date,login_count,rn, wether_e,\n        SUM(wether_e) OVER (ORDER BY rn) AS lab\n    FROM whether_eq),\nconsecutive_logins_num AS (\n    SELECT userid,login_date,login_count,rn, wether_e,lab,\n        (SELECT (CASE WHEN max(COUNT(*))<3 THEN 0 ELSE 1 END)\n        FROM numbered_sequence b\n        WHERE b.rn BETWEEN a.rn - 6 AND a.rn\n        AND b.userid=a.userid\n        GROUP BY b. lab) AS cnt\n    FROM numbered_sequence a)\nSELECT login_date,SUM(cnt) AS cont3_num\nFROM consecutive_logins_num\nWHERE login_date>=(SELECT MIN(login_date) FROM all_dates)+6\nGROUP BY login_date\nORDER BY login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngrouped = df.groupby(\"userid\")\naligned_dates = pd.date_range(start=df[\"ts\"].min(), end=df[\"ts\"].max(), freq='D')\nuser_date_wether_con3days = []\nfor uid, group in grouped:\n    group = group.drop_duplicates('ts')\n    aligned_group = group.set_index(\"ts\").reindex(aligned_dates)\n    consecutive_logins = aligned_group.rolling(window=7)\n    n = 0\n    date_wether_con3days = []\n    for r in consecutive_logins:\n        n += 1\n        if n<7:\n            continue\n        else:\n            ds = r['userid'].isna().cumsum()\n            cont_login_times = r.groupby(ds).userid.count().max()\n            wether_cont3days = 1 if cont_login_times>=3 else 0\n            date_wether_con3days.append(wether_cont3days)\n    user_date_wether_con3days.append(date_wether_con3days)\narr = np.array(user_date_wether_con3days)\nday7_cont3num = np.sum(arr,axis=0)\nresult = pd.DataFrame({'dt':aligned_dates[6:],'cont3_num':day7_cont3num})\n\n4. Calculate the next-day retention rate of new users per day\nUser login data table\n\nSPL\n\nA2: Group by user; record the first login date and check whether the user logs in the next day.\nA3: Calculate the next-day retention rate based on the login date of the next day.\nSQL\nWITH first_login AS (\n    SELECT userid, MIN(TRUNC(ts)) AS first_login_date\n    FROM login_data\n    GROUP BY userid),\nnext_day_login AS (\n    SELECT DISTINCT(fl.userid), fl.first_login_date, TRUNC(ld.ts) AS next_day_login_date\n    FROM first_login fl\n    LEFT JOIN login_data ld ON fl.userid = ld.userid\n    WHERE TRUNC(ld.ts) = fl.first_login_date + 1),\nday_new_users AS(\n    SELECT first_login_date,COUNT(*) AS new_user_num\n    FROM first_login\n    GROUP BY first_login_date),\nnext_new_users AS(\n    SELECT next_day_login_date, COUNT(*) AS next_user_num\n    FROM next_day_login\n    GROUP BY next_day_login_date),\nall_date AS(\n    SELECT DISTINCT(TRUNC(ts)) AS login_date\n    FROM login_data)\nSELECT all_date.login_date+1 AS dt,dn. new_user_num,nn. next_user_num,\n    (CASE \n        WHEN nn. next_day_login_date IS NULL \n        THEN 0 \n        ELSE nn.next_user_num \n    END)/dn.new_user_num AS ret_rate\nFROM all_date\n    JOIN day_new_users dn ON all_date.login_date=dn.first_login_date\n    LEFT JOIN next_new_users nn ON dn.first_login_date+1=nn. next_day_login_date\nORDER BY all_date.login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngp = df.groupby('userid')\nrow = []\nfor uid,g in gp:\n    fst_dt = g.iloc[0].ts\n    sec_dt = fst_dt + pd.Timedelta(days=1)\n    all_dt = g.ts.values\n    wether_sec_login = sec_dt in all_dt\n    row.append([uid,fst_dt,sec_dt,wether_sec_login])\nuser_wether_ret_df = pd.DataFrame(row,columns=['userid','fst_dt','sec_dt','wether_sec_login'])\nresult = user_wether_ret_df.groupby('sec_dt').apply(lambda x:x['wether_sec_login'].sum()/len(x))\n\n5. Calculate the increase of stock price on the day when it is higher than those on the previous and next 5 days\nStock price data table\n\nSPL\n\nA2: The position where the stock price is higher than those of the previous and next 5 days.\nA3: Calculate the increase at that time.\nSQL\nSELECT closing/closing_pre-1 AS raise\nFROM(\n    SELECT dt, closing, ROWNUM AS rn,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING) AS max_pre,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 1 FOLLOWING AND 5 FOLLOWING) AS max_suf,\n        LAG(closing,1) OVER (ORDER BY dt) AS closing_pre\n    FROM stock)\nWHERE rn>5 AND rn<=(select count(*) FROM stock)-5\n    AND CLOSING>max_pre AND CLOSING>max_suf;\n\nPython\nstock_price_df = pd.read_csv('STOCK.csv')\nprice_increase_list = []\nfor i in range(5, len(stock_price_df)-5):\n    if stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i-5:i]) and \\\n    stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i+1:i+6]):\n        price_increase = stock_price_df['CLOSING'][i] / stock_price_df['CLOSING'][i-1]-1\n        price_increase_list.append(price_increase)\nresult = price_increase_list\n\nesProc SPL is open-source and here's the Open-Source Address.",
    "url": "https://www.indiehackers.com/post/data-analysis-showdown-comparing-sql-python-and-esproc-spl-dd4c31d7ce",
    "crawl_time": "2025-02-10 06:42:24",
    "translated_content": "Talk is cheap; let’s show the codes.\n1. User Session Count\nUser behavior data table\n\nA session is considered over if a user does not take any action within 10 minutes, or if they do not log in within 5 minutes after logging out. Calculate the number of sessions for each user.\nSPL\n\nSQL\nWITH login_data AS (\n    SELECT userid, action_type, action_time,\n        LAG(action_time) OVER (PARTITION BY userid ORDER BY action_time) AS prev_time,\n        LAG(action_type) OVER (PARTITION BY userid ORDER BY action_time) AS prev_action\n    FROM session_data)\nSELECT userid, COUNT(*) AS session_count\nFROM (\n    SELECT userid, action_type, action_time, prev_time, prev_action,\n        CASE\n            WHEN prev_time IS NULL OR (action_time - prev_time) > 60 \n                OR (prev_action = 'exit' AND (action_time - prev_time) > 300 )\n            THEN 1\n            ELSE 0\n        END AS is_new_session\n    FROM login_data)\nWHERE is_new_session = 1\nGROUP BY userid;\n\nPython\nlogin_data = pd.read_csv(\"session_data.csv\")\nlogin_data['action_time'] = pd.to_datetime(login_data['action_time'])\ngrouped = login_data.groupby(\"userid\")\nsession_count = {}\nfor uid, sub_df in grouped:\n    session_count[uid] = 0\n    start_index = 0\n    for i in range(1, len(sub_df)):\n        current = sub_df.iloc[i]\n        last = sub_df.iloc[start_index]\n        last_action = last['action_type']\n        if (current[\"action_time\"] - last[\"action_time\"]).seconds > 600 or \\\n            (last_action==\"exit\" and (current[\"action_time\"] - last[\"action_time\"]).seconds > 300):\n            session_count[uid] += 1\n        start_index = i\n    session_count[uid] += 1\nsession_cnt = pd.DataFrame(list(session_count.items()), columns=['UID', 'session_count'])\n\n2. Count the players who score 3 times in a row within 1 minute\nScore table of a ball game\n\nSPL\n\nSQL\nWITH numbered_scores AS (\n    SELECT team, player, play_time, score,\n        ROW_NUMBER() OVER (ORDER BY play_time) AS rn\n    FROM ball_game)\nSELECT DISTINCT s1.player\nFROM numbered_scores s1\n    JOIN numbered_scores s2 ON s1.player = s2.player AND s1.rn = s2.rn - 1\n    JOIN numbered_scores s3 ON s1.player = s3.player AND s1.rn = s3.rn - 2\nWHERE (s3.play_time - s1.play_time) <60 ;\n\nPython\ndf = pd.read_csv(\"ball_game.csv\")\ndf[\"play_time\"] = pd.to_datetime(df[\"play_time\"])\nresult_players = []\nplayer = None\nstart_index = 0\nconsecutive_scores = 0\nfor i in range(len(df)-2):\n    current = df.iloc[i]\n    if player != current[\"player\"]:\n        player = current[\"player\"]\n        consecutive_scores = 1\n    else:\n        consecutive_scores += 1\n    last2 = df.iloc[i-2] if i >=2 else None\n    if consecutive_scores >= 3 and (current['play_time'] - last2['play_time']).seconds < 60:\n        result_players.append(player)\nresult_players = list(set(result_players))\n\n3. Calculate the number of users who are active for three consecutive days within every 7 days\nUser login data table\n\nSPL\n\nSQL\nWITH all_dates AS (\n    SELECT DISTINCT TRUNC(ts) AS login_date\n    FROM login_data),\nuser_login_counts AS (\n    SELECT userid, TRUNC(ts) AS login_date, \n        (CASE WHEN COUNT(*)>=1 THEN 1 ELSE 0 END) AS login_count\n    FROM login_data\n    GROUP BY userid, TRUNC(ts)),\nwhether_login AS (\n    SELECT u.userid, ad.login_date, NVL(ulc.login_count, 0) AS login_count\n    FROM all_dates ad\n    CROSS JOIN (\n        SELECT DISTINCT userid\n        FROM login_data) u\n    LEFT JOIN user_login_counts ulc\n    ON u.userid = ulc.userid\n    AND ad.login_date = ulc.login_date\n    ORDER BY u.userid, ad.login_date),\nwhether_login_rn AS (\n    SELECT userid,login_date,login_count,ROWNUM AS rn \n    FROM whether_login),\nwhether_eq AS(\n    SELECT userid,login_date,login_count,rn,\n        (CASE \n            WHEN LAG(login_count,1) OVER (ORDER BY rn)= login_count \n                AND login_count =1 AND LAG(userid,1) OVER (ORDER BY rn)=userid \n            THEN 0 \n            ELSE 1 \n        END) AS wether_e \n    FROM whether_login_rn\n),\nnumbered_sequence AS (\n    SELECT userid,login_date,login_count,rn, wether_e,\n        SUM(wether_e) OVER (ORDER BY rn) AS lab\n    FROM whether_eq),\nconsecutive_logins_num AS (\n    SELECT userid,login_date,login_count,rn, wether_e,lab,\n        (SELECT (CASE WHEN max(COUNT(*))<3 THEN 0 ELSE 1 END)\n        FROM numbered_sequence b\n        WHERE b.rn BETWEEN a.rn - 6 AND a.rn\n        AND b.userid=a.userid\n        GROUP BY b. lab) AS cnt\n    FROM numbered_sequence a)\nSELECT login_date,SUM(cnt) AS cont3_num\nFROM consecutive_logins_num\nWHERE login_date>=(SELECT MIN(login_date) FROM all_dates)+6\nGROUP BY login_date\nORDER BY login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngrouped = df.groupby(\"userid\")\naligned_dates = pd.date_range(start=df[\"ts\"].min(), end=df[\"ts\"].max(), freq='D')\nuser_date_wether_con3days = []\nfor uid, group in grouped:\n    group = group.drop_duplicates('ts')\n    aligned_group = group.set_index(\"ts\").reindex(aligned_dates)\n    consecutive_logins = aligned_group.rolling(window=7)\n    n = 0\n    date_wether_con3days = []\n    for r in consecutive_logins:\n        n += 1\n        if n<7:\n            continue\n        else:\n            ds = r['userid'].isna().cumsum()\n            cont_login_times = r.groupby(ds).userid.count().max()\n            wether_cont3days = 1 if cont_login_times>=3 else 0\n            date_wether_con3days.append(wether_cont3days)\n    user_date_wether_con3days.append(date_wether_con3days)\narr = np.array(user_date_wether_con3days)\nday7_cont3num = np.sum(arr,axis=0)\nresult = pd.DataFrame({'dt':aligned_dates[6:],'cont3_num':day7_cont3num})\n\n4. Calculate the next-day retention rate of new users per day\nUser login data table\n\nSPL\n\nA2: Group by user; record the first login date and check whether the user logs in the next day.\nA3: Calculate the next-day retention rate based on the login date of the next day.\nSQL\nWITH first_login AS (\n    SELECT userid, MIN(TRUNC(ts)) AS first_login_date\n    FROM login_data\n    GROUP BY userid),\nnext_day_login AS (\n    SELECT DISTINCT(fl.userid), fl.first_login_date, TRUNC(ld.ts) AS next_day_login_date\n    FROM first_login fl\n    LEFT JOIN login_data ld ON fl.userid = ld.userid\n    WHERE TRUNC(ld.ts) = fl.first_login_date + 1),\nday_new_users AS(\n    SELECT first_login_date,COUNT(*) AS new_user_num\n    FROM first_login\n    GROUP BY first_login_date),\nnext_new_users AS(\n    SELECT next_day_login_date, COUNT(*) AS next_user_num\n    FROM next_day_login\n    GROUP BY next_day_login_date),\nall_date AS(\n    SELECT DISTINCT(TRUNC(ts)) AS login_date\n    FROM login_data)\nSELECT all_date.login_date+1 AS dt,dn. new_user_num,nn. next_user_num,\n    (CASE \n        WHEN nn. next_day_login_date IS NULL \n        THEN 0 \n        ELSE nn.next_user_num \n    END)/dn.new_user_num AS ret_rate\nFROM all_date\n    JOIN day_new_users dn ON all_date.login_date=dn.first_login_date\n    LEFT JOIN next_new_users nn ON dn.first_login_date+1=nn. next_day_login_date\nORDER BY all_date.login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngp = df.groupby('userid')\nrow = []\nfor uid,g in gp:\n    fst_dt = g.iloc[0].ts\n    sec_dt = fst_dt + pd.Timedelta(days=1)\n    all_dt = g.ts.values\n    wether_sec_login = sec_dt in all_dt\n    row.append([uid,fst_dt,sec_dt,wether_sec_login])\nuser_wether_ret_df = pd.DataFrame(row,columns=['userid','fst_dt','sec_dt','wether_sec_login'])\nresult = user_wether_ret_df.groupby('sec_dt').apply(lambda x:x['wether_sec_login'].sum()/len(x))\n\n5. Calculate the increase of stock price on the day when it is higher than those on the previous and next 5 days\nStock price data table\n\nSPL\n\nA2: The position where the stock price is higher than those of the previous and next 5 days.\nA3: Calculate the increase at that time.\nSQL\nSELECT closing/closing_pre-1 AS raise\nFROM(\n    SELECT dt, closing, ROWNUM AS rn,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING) AS max_pre,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 1 FOLLOWING AND 5 FOLLOWING) AS max_suf,\n        LAG(closing,1) OVER (ORDER BY dt) AS closing_pre\n    FROM stock)\nWHERE rn>5 AND rn<=(select count(*) FROM stock)-5\n    AND CLOSING>max_pre AND CLOSING>max_suf;\n\nPython\nstock_price_df = pd.read_csv('STOCK.csv')\nprice_increase_list = []\nfor i in range(5, len(stock_price_df)-5):\n    if stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i-5:i]) and \\\n    stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i+1:i+6]):\n        price_increase = stock_price_df['CLOSING'][i] / stock_price_df['CLOSING'][i-1]-1\n        price_increase_list.append(price_increase)\nresult = price_increase_list\n\nesProc SPL is open-source and here's the Open-Source Address.",
    "category": "产品"
  },
  {
    "title": "Getting out of the freelancing game by building a $100k+ MRR Shopify app portfolio",
    "content": "CompanyKaching AppzFounderErikas MališauskasRevenue>$100K a monthErikas Mališauskas was a successful freelancer who wanted to build something for himself. After a few failed attempts, he had a small success which got acquired. He then used that money to get a portfolio of Shopify apps off the ground.Now, the Kaching Appz portfolio is bringing in a 6-figure MRR. Here's Erikas on how he did it. 👇ContentsFrom freelancer to indie hackerComing up with an ideaShopify tech stackPricing rightGrowing with a focus on productParting adviceWhat's next?From freelancer to indie hackerFor most of my career, I was quite a successful freelance designer, and I had a design agency as well. Financially, I was doing well, but I really wanted to work on my own products instead of doing client work. I believed that was the way to scale my income indefinitely.So, since 2018, I’ve been trying to build successful products to replace my clients. Several of my projects failed before I launched my first success in 2021. It was a Shopify app that I scaled to $6.5k MRR and sold for $250k.I used that experience to launch another app, Kaching Bundle Quantity Breaks, which is currently at a 6-figure MRR. And we’re building more Shopify apps under the Kaching Appz name.I also have other hobby projects such as my screenshot editor, PimpMySnap.Coming up with an ideaWhen I was freelancing, a lot of my clients were doing e-commerce on Shopify, so I got to know the Shopify ecosystem from the inside.When I stumbled upon the Shopify partners page, I saw an interesting banner stating that the top 25% Shopify app developers earn $272K annually. Having used quite a few Shopify apps myself, I knew I could do better with all of my UX experience.I dedicated significant time to brainstorming ideas, analyzing the competitive landscape, and researching popular app store search queries. My main criteria were:Development complexity - I wanted a product that could be built quickly so I could launch, test, and iterate rapidly.Target audience size - the solution needed to be scalable, addressing a widespread problem that every store could benefit from, rather than catering to a niche audience of just 30 people.Competition landscape - competing against apps with hundreds of 5-star reviews would be incredibly challenging, so I aimed to find a space with less established competition.Eventually, I decided I will make a simple app to add icons to a store. Whether it’s product feature icons or store guarantees, I made it easy to create a block with the app and place it anywhere. There were only a couple of apps doing that and both were a bit outdated, with fewer than 30 reviews at the time. It seemed like a good opportunity, so I drafted the app's design and found a local tech cofounder who could turn my designs into a working product. We spent a few weeks building a working prototype and launching it in the Shopify app store.Shopify tech stackOur Shopify apps have two main parts: an iframe embedded in the Shopify admin for merchants, and a storefront widget integrated right into the shop’s theme for customers. For our Bundles app admin, we started with Ruby on Rails, React, Postgres, and Heroku — mostly because Shopify offered great Rails support at the time, and it fit well with our skillset. These days, we’re moving to Remix and TypeScript for new apps, since that’s now Shopify’s recommended approach.On the storefront side, we use Svelte. It compiles into a small web component that doesn’t require a runtime, which helps keep load times low — and we can reuse that same component in the admin area for previews. We store all configuration details in Shopify metafields instead of making server calls, which helps maintain speed and reliability. For discounts, we rely on Shopify Functions, which run our TypeScript code millions of times each day on Shopify’s infrastructure. This setup makes it easy to scale to tens of thousands of users without driving up costs, and we never have to worry about Black Friday or other high-traffic surges.When choosing our tools, we consider what Shopify recommends, whether we can reuse our existing tech, our team’s familiarity with it, and how mature it is. Following these guidelines helps us build apps that are fast, reliable, and fit seamlessly into Shopify’s ecosystem.Pricing rightAll of our apps are subscription based and have a few tiers based on usage/value. We only earn more when our users earn more.For example, our Basic plan costs $14.99 and it comes with all of the features. Once merchants get $1,000 additional revenue through our app, they’re upgraded to our Scale plan at $29.00. It’s a tremendous 34x return on investment for them. We have merchants making literally millions of dollars with our app paying only $59.99.I think testing pricing is crucial for any business. I remember one of my friends in eCom told a story about how they increased their pricing by 30% and the conversion rate didn’t change at all. But I hate raising prices “just because you can”, so I always introduce a big new feature before a price increase. I also always grandfather our existing customers in so they never get a price increase.We’re an extremely lean company. We always keep our profitability at 90%. We still have no office, no in-house employees, no agencies we’re working with.I think most people start spending more when they earn more, just because they can. Fancy offices, over-hiring, etc. Then, company effectiveness drops, and slowly, you become just another corporation. I hate that. We’re trying to hustle as hard as our first day with $0 MRR.Growing with a focus on productProbably the biggest challenge for every indie maker is getting their first users. For us, one thing was a huge success: manual outreach via Facebook groups.We just shilled our app in dropshipping groups. We gave it out for free. Some of the posts went semi-viral which resulted in our app being one of the most trending apps in the Shopify app store, and that brought us even more installs. We managed to get 1,000 users in the first month.I learned that doing things that don’t scale is the key in the beginning. We reached out to every user manually, did all the customer support ourselves, and gathered a lot of feedback. That helped us shape our product before introducing monetization.After all, we're a product-led company with no marketing team. We believe that once you have an initial userbase, you just need to build a great product and make your users happy. Word of mouth will kick in. Our app has been featured by quite a few e-commerce gurus in various videos and courses without us even knowing about it.We currently get around 15k installs a month. 50% of that comes from word of mouth — we can attribute it to the branded search keyword “kaching”. Another 40% comes from other organic keywords, and 10% comes from affiliate partnerships and ads.Parting adviceBuild what you knowFind a niche you’re comfortable with and build around that. It’s always easier to build for yourself or someone close who you know really well. Market knowledge is a game changer. I don’t think I would have been successful without previous experience in Shopify.Don't over-researchDon’t overthink researching things and finding a perfect fit. I don’t believe in a “perfect idea”. Instead of spending weeks researching, just give it a try, build an MVP in a few weeks and see how it goes. Even if it’s a failure, you will learn so much more from the process than from the research.Focus on the customerShipping fast and focusing on customer feedback is the key. A few months ago we even introduced a feature suggestion board for every app of ours. Almost all the things that we end up building now come from there.BootstrapIf you’re looking for financial freedom, avoid raising money from VCs. You will never be free with a VC over your head. A $10K MRR product could be a life changer for most founders, enabling financial freedom. But no VC firm would be happy with $10K MRR, so they wouldn’t ever let you take the profits. You would be forced to spend everything on growth, raise more money, and then burn that for growth. Bootstrapping makes more sense.Find a cofounderOne of my mistakes early on was hiring a development contractor instead of having a cofounder. Soon enough, I realized that it’s too hard to manage them. It was too much time and money. So, after a couple of months struggling with the contractor and not launching the app, I found a cofounder. We launched an app with him in just a few weeks and we’re still working together more than four years later.Read theseI believe that every founder has to read Paul Graham’s essays and Startup Playbook by Sam Altman.What's next?My goal is to reach a $1M MRR with KachingAppz. Optimistically, I believe we could hit it before 2026. Our vision is having a portfolio of Shopify apps in the upsell/discount category. All of those apps will be well integrated and cross promoted.Besides that, I really want to personally run a startup studio where all of my ideas could come to reality. I already have a couple of side projects like PimpMySnap. I’m also building a couple of Webflow apps as we speak. I’m still curious about building stuff, therefore I will continue doing that.I just need a proper system for this because my personal time is very limited and currently all of the projects I do require a lot of involvement from me.If you want to follow along, I’m sharing my entrepreneurship journey on my X. There, you will find all my ideas, daily struggles, failures and successes.Indie Hackers Newsletter: Subscribe to get the latest stories, trends, and insights for indie hackers in your inbox 3x/week.",
    "url": "https://www.indiehackers.com/post/tech/getting-out-of-the-freelancing-game-by-building-a-100k-mrr-shopify-app-portfolio-qdReVAgLjz6EpW4OrJSI",
    "crawl_time": "2025-02-10 06:42:27",
    "translated_content": "CompanyKaching AppzFounderErikas MališauskasRevenue>$100K a monthErikas Mališauskas was a successful freelancer who wanted to build something for himself. After a few failed attempts, he had a small success which got acquired. He then used that money to get a portfolio of Shopify apps off the ground.Now, the Kaching Appz portfolio is bringing in a 6-figure MRR. Here's Erikas on how he did it. 👇ContentsFrom freelancer to indie hackerComing up with an ideaShopify tech stackPricing rightGrowing with a focus on productParting adviceWhat's next?From freelancer to indie hackerFor most of my career, I was quite a successful freelance designer, and I had a design agency as well. Financially, I was doing well, but I really wanted to work on my own products instead of doing client work. I believed that was the way to scale my income indefinitely.So, since 2018, I’ve been trying to build successful products to replace my clients. Several of my projects failed before I launched my first success in 2021. It was a Shopify app that I scaled to $6.5k MRR and sold for $250k.I used that experience to launch another app, Kaching Bundle Quantity Breaks, which is currently at a 6-figure MRR. And we’re building more Shopify apps under the Kaching Appz name.I also have other hobby projects such as my screenshot editor, PimpMySnap.Coming up with an ideaWhen I was freelancing, a lot of my clients were doing e-commerce on Shopify, so I got to know the Shopify ecosystem from the inside.When I stumbled upon the Shopify partners page, I saw an interesting banner stating that the top 25% Shopify app developers earn $272K annually. Having used quite a few Shopify apps myself, I knew I could do better with all of my UX experience.I dedicated significant time to brainstorming ideas, analyzing the competitive landscape, and researching popular app store search queries. My main criteria were:Development complexity - I wanted a product that could be built quickly so I could launch, test, and iterate rapidly.Target audience size - the solution needed to be scalable, addressing a widespread problem that every store could benefit from, rather than catering to a niche audience of just 30 people.Competition landscape - competing against apps with hundreds of 5-star reviews would be incredibly challenging, so I aimed to find a space with less established competition.Eventually, I decided I will make a simple app to add icons to a store. Whether it’s product feature icons or store guarantees, I made it easy to create a block with the app and place it anywhere. There were only a couple of apps doing that and both were a bit outdated, with fewer than 30 reviews at the time. It seemed like a good opportunity, so I drafted the app's design and found a local tech cofounder who could turn my designs into a working product. We spent a few weeks building a working prototype and launching it in the Shopify app store.Shopify tech stackOur Shopify apps have two main parts: an iframe embedded in the Shopify admin for merchants, and a storefront widget integrated right into the shop’s theme for customers. For our Bundles app admin, we started with Ruby on Rails, React, Postgres, and Heroku — mostly because Shopify offered great Rails support at the time, and it fit well with our skillset. These days, we’re moving to Remix and TypeScript for new apps, since that’s now Shopify’s recommended approach.On the storefront side, we use Svelte. It compiles into a small web component that doesn’t require a runtime, which helps keep load times low — and we can reuse that same component in the admin area for previews. We store all configuration details in Shopify metafields instead of making server calls, which helps maintain speed and reliability. For discounts, we rely on Shopify Functions, which run our TypeScript code millions of times each day on Shopify’s infrastructure. This setup makes it easy to scale to tens of thousands of users without driving up costs, and we never have to worry about Black Friday or other high-traffic surges.When choosing our tools, we consider what Shopify recommends, whether we can reuse our existing tech, our team’s familiarity with it, and how mature it is. Following these guidelines helps us build apps that are fast, reliable, and fit seamlessly into Shopify’s ecosystem.Pricing rightAll of our apps are subscription based and have a few tiers based on usage/value. We only earn more when our users earn more.For example, our Basic plan costs $14.99 and it comes with all of the features. Once merchants get $1,000 additional revenue through our app, they’re upgraded to our Scale plan at $29.00. It’s a tremendous 34x return on investment for them. We have merchants making literally millions of dollars with our app paying only $59.99.I think testing pricing is crucial for any business. I remember one of my friends in eCom told a story about how they increased their pricing by 30% and the conversion rate didn’t change at all. But I hate raising prices “just because you can”, so I always introduce a big new feature before a price increase. I also always grandfather our existing customers in so they never get a price increase.We’re an extremely lean company. We always keep our profitability at 90%. We still have no office, no in-house employees, no agencies we’re working with.I think most people start spending more when they earn more, just because they can. Fancy offices, over-hiring, etc. Then, company effectiveness drops, and slowly, you become just another corporation. I hate that. We’re trying to hustle as hard as our first day with $0 MRR.Growing with a focus on productProbably the biggest challenge for every indie maker is getting their first users. For us, one thing was a huge success: manual outreach via Facebook groups.We just shilled our app in dropshipping groups. We gave it out for free. Some of the posts went semi-viral which resulted in our app being one of the most trending apps in the Shopify app store, and that brought us even more installs. We managed to get 1,000 users in the first month.I learned that doing things that don’t scale is the key in the beginning. We reached out to every user manually, did all the customer support ourselves, and gathered a lot of feedback. That helped us shape our product before introducing monetization.After all, we're a product-led company with no marketing team. We believe that once you have an initial userbase, you just need to build a great product and make your users happy. Word of mouth will kick in. Our app has been featured by quite a few e-commerce gurus in various videos and courses without us even knowing about it.We currently get around 15k installs a month. 50% of that comes from word of mouth — we can attribute it to the branded search keyword “kaching”. Another 40% comes from other organic keywords, and 10% comes from affiliate partnerships and ads.Parting adviceBuild what you knowFind a niche you’re comfortable with and build around that. It’s always easier to build for yourself or someone close who you know really well. Market knowledge is a game changer. I don’t think I would have been successful without previous experience in Shopify.Don't over-researchDon’t overthink researching things and finding a perfect fit. I don’t believe in a “perfect idea”. Instead of spending weeks researching, just give it a try, build an MVP in a few weeks and see how it goes. Even if it’s a failure, you will learn so much more from the process than from the research.Focus on the customerShipping fast and focusing on customer feedback is the key. A few months ago we even introduced a feature suggestion board for every app of ours. Almost all the things that we end up building now come from there.BootstrapIf you’re looking for financial freedom, avoid raising money from VCs. You will never be free with a VC over your head. A $10K MRR product could be a life changer for most founders, enabling financial freedom. But no VC firm would be happy with $10K MRR, so they wouldn’t ever let you take the profits. You would be forced to spend everything on growth, raise more money, and then burn that for growth. Bootstrapping makes more sense.Find a cofounderOne of my mistakes early on was hiring a development contractor instead of having a cofounder. Soon enough, I realized that it’s too hard to manage them. It was too much time and money. So, after a couple of months struggling with the contractor and not launching the app, I found a cofounder. We launched an app with him in just a few weeks and we’re still working together more than four years later.Read theseI believe that every founder has to read Paul Graham’s essays and Startup Playbook by Sam Altman.What's next?My goal is to reach a $1M MRR with KachingAppz. Optimistically, I believe we could hit it before 2026. Our vision is having a portfolio of Shopify apps in the upsell/discount category. All of those apps will be well integrated and cross promoted.Besides that, I really want to personally run a startup studio where all of my ideas could come to reality. I already have a couple of side projects like PimpMySnap. I’m also building a couple of Webflow apps as we speak. I’m still curious about building stuff, therefore I will continue doing that.I just need a proper system for this because my personal time is very limited and currently all of the projects I do require a lot of involvement from me.If you want to follow along, I’m sharing my entrepreneurship journey on my X. There, you will find all my ideas, daily struggles, failures and successes.Indie Hackers Newsletter: Subscribe to get the latest stories, trends, and insights for indie hackers in your inbox 3x/week.",
    "category": "其他"
  },
  {
    "title": "From viral side project to a $5M/yr B2B AI platform",
    "content": "CompanyChatbaseFounderYasser ElsaidRevenue>$417K a monthTwo years ago, Yasser Elsaid launched Chatbase, and within just six months, he hit $64k MRR. Now, as he relaunches it on his two-year anniversary, he's at $5M ARR.Here's Yasser on how he did it. 👇ContentsAccidentally viralPivoting the MVPTwo years laterOrganic growthParting adviceWhat's next?Accidentally viralIn February 2023, I was in my final year of university, and I had a full course load. I didn’t receive a return offer from my Meta internship, which, in hindsight, was the best thing that ever happened to me.I was inspired by some big names in the indie hacking space that were getting into AI, like Pieter Levels. I then started exploring how to build useful tools using OpenAI’s API.The idea for Chatbase started as “ChatGPT for your PDFs,” a simple tool that allowed users to upload a document and chat with its contents. I tweeted about it to my 16 followers, and to my surprise, it went viral.I realized I had hit on something with huge potential, and within weeks, I decided to make Chatbase my full-time focus, even at the cost of failing two of my final classes.It was worth it.Pivoting the MVPBuilding the MVP for Chatbase took about two months. The first version was simple but effective: Upload a PDF and the tool would create a chatbot that could answer questions based on the document’s content.I used React, Next.js, Supabase, OpenAI’s API, LangChain, and Pinecone to bring it to life. It was a lean, cost-effective stack that allowed me to move quickly.The viral success of my launch tweet showed me that there was real demand. However, I quickly realized that focusing on PDFs alone wouldn’t be enough. I needed to stay ahead of the competition.When Chatbase launched, it was the first “Chat with PDF” SaaS tool. Within months, competitors flooded the space, and it became clear that differentiation was critical.This realization led to a pivot toward building a platform for customer-facing AI agents, which has a much higher technical barrier to entry, a larger market, and greater long-term potential.Two years laterNow, Chatbase is a platform that allows businesses to create AI-powered customer support agents. These agents aren’t just bots that answer questions — they’re capable of taking actions, like updating subscriptions, rescheduling appointments, or handling customer requests in real time.It is a subscription-based platform with tiered pricing, starting from $40/month plan up to an enterprise plan. The bulk of our revenue comes from higher-tier plans, where businesses use our API integrations and advanced features to create fully-customized AI agents.Revenue growth has been consistent, scaling from $64,000 MRR in May 2023 to over $5M ARR today. Much of this growth has been driven by the shift from simple chatbots to AI agents that can perform actions, which adds tremendous value for businesses.Organic growthThe initial launch of Chatbase went viral thanks to the perfect storm of being early in the AI space and riding the ChatGPT hype.Tweets and posts on platforms like Indie Hackers, Product Hunt, Reddit, and AI directories drove the first wave of users. AI influencers also shared Chatbase organically, which helped immensely.After the initial wave, we focused on long-term growth strategies:SEO and content marketing: Building a library of resources around AI customer support and chatbot implementation.Direct partnerships: Collaborating with companies for case studies and adding logos to our site for social proof.Customer feedback loops: Listening closely to users and iterating quickly to add features that meet their needs.These efforts have paid off, and today we’re serving over 9,000 active subscriptions.Parting adviceLaunch early and often: The sooner you get something in front of users, the sooner you can learn what works.Listen to your customers: They’ll tell you what they need if you’re paying attention.Focus on high-barrier opportunities: Spaces with low competition but high potential are where you can really thrive.Build in public: Sharing your journey not only helps you grow an audience but also connects you with like-minded people who can support you.What's next?As we relaunch Chatbase on its two-year anniversary, February 4, 2025, our goal is to cement our position as the leading platform for building customer-facing AI agents. We plan to:Expand integrations with tools businesses already use, like Stripe and Cal.com.Have better functionality around the AI model for specific use cases like SaaS support.You can follow along on X and LinkedIn. And check out Chatbase!Indie Hackers Newsletter: Subscribe to get the latest stories, trends, and insights for indie hackers in your inbox 3x/week.",
    "url": "https://www.indiehackers.com/post/tech/from-viral-side-project-to-a-5m-yr-b2b-ai-platform-TpbhTVyp1sjBk0uzo4tR",
    "crawl_time": "2025-02-10 06:42:31",
    "translated_content": "CompanyChatbaseFounderyAsser ElsaidRevenue>每月41.7万美元，Yasser Elsaid推出了Chatbase，在短短六个月内，他达到了6.4K MRR。现在，随着他在他两周年纪念日重新启动时，他的Yasser为500万美元。 contentsAccccccccccccccccced proverpivotivoting Mvptwo年的培训疗法培训专业建议下一个是什么？我没有收到我的Meta实习的回报报价，事后看来，这是我发生过的最好的事情。我的灵感来自独立黑客入侵空间中的一些知名人士，这些空间正在进入AI，例如Pieter级别。然后，我开始探索如何使用OpenAI的API构建有用的工具。chatbase的想法最初是“用于您的PDFS的ChatGpt”，这是一种简单的工具，允许用户上传文档并与其内容聊天。我向我的16位追随者发了推文，令我惊讶的是，我意识到我已经击中了一些潜力巨大的事情，几周之内，我决定将chatbase成为我的全职重点，即使是为了失败的代价我的最后两个课程值得。这是值得的。第一个版本很简单但有效：上传PDF，该工具将创建一个聊天机器人，可以根据文档的内容回答问题。我使用了React，Next.js，Supabase，Openai的API，Langchain和Pinecone，以将其栩栩如生。这是一个精简，具有成本效益的堆栈，使我能够快速移动。我的发布推文的病毒成功表明我有真正的需求。但是，我很快意识到，仅专注于PDF是不够的。我需要在比赛中保持领先。几个月之内，竞争对手淹没了这个空间，很明显差异化至关重要。这导致了建立面向客户AI代理平台的枢纽，该平台具有更高的技术障碍，更大的市场和更大的市场障碍长期的潜力。两年后，Chatbase是一个平台，允许企业创建AI驱动的客户支持代理商。这些代理不仅可以回答问题的机器人 - 它们能够采取行动，例如更新订阅，重新安排约会或实时处理客户请求。计划企业计划。我们的大部分收入来自高层计划，在该计划中，企业使用我们的API集成和高级功能来创建完全定制的AI代理。收益的增长一直保持一致，从2023年5月的64,000美元扩展到今天的500万美元ARR。这种增长的大部分是由从简单的聊天机器人转移到可以执行动作的AI代理的转变所驱动的，这为业务增添了巨大的价值。有机增长，Chatbase的最初发射逐渐流行，这要归功于在AI空间早期和骑马的早期风暴Chatgpt Hype.tweets和诸如Indie Hackers，Product Hunt，Reddit和AI目录等平台上的帖子驱动了第一波用户。人工智能影响者还有机地共享了chatbase，这对最初的浪潮产生了极大的帮助。我们专注于长期增长策略：SEO和内容营销：围绕AI客户支持和聊天机器人实施建立资源库。直接合作伙伴关系：与公司与公司合作，以争取为公司合作而进行。案例研究并将徽标添加到我们的网站以获得社交证明。customer反馈循环：仔细聆听用户并迅速迭代以添加满足他们需求的功能。这些努力得到了回报，今天我们正在为9,000多个活动提供服务。早期和经常：您越早在用户面前得到一些东西，您就可以越早了解有效的作用。向客户说：他们会告诉您他们需要注意的是什么。在竞争较低但潜力较大的情况下，您可以真正蓬勃发展。在公共场合建立：分享您的旅程不仅可以帮助您发展观众，还可以将您与志趣相投的人联系在一起。接下来是什么？接下来是什么？ 2025年2月4日，两周年纪念日，我们的目标是巩固我们作为建立面向客户AI代理商的领先平台的地位。我们计划：扩展与已经使用的工具相集成的集成，例如Stripe和Cal.com。在AI模型周围具有更好的功能，例如SaaS Support等特定用例。您可以在X和LinkedIn上遵循。并查看chatbase！独立黑客通讯：订阅以获取收件箱3x/周的独立黑客的最新故事，趋势和见解。",
    "category": "技术"
  },
  {
    "title": "Freeing Data Scientists from Heavy Coding: A New Era of Automation",
    "content": "SQL is wasting lives of data scientists\nSQL is difficult to write\nNearly all data scientists use SQL for data exploration and analysis. SQL appears deceptively simple and offers a certain degree of interactivity, making it a seemingly ideal choice for these purposes.\nTo perform filtering and grouping operations, for example, SQL just needs one line of code:\nselect id,name from T where id=1\nselect area,sum(amount) from T group by area\n\nBut this is only limited to simple cases. When the computation becomes complex, SQL code becomes complicated, too. For example, to count the longest consecutive rising days of each stock, the SQL statement is as follows:\nSELECT CODE, MAX(con_rise) AS longest_up_days\nFROM (\n    SELECT CODE, COUNT(*) AS con_rise\n    FROM (\n        SELECT CODE, DT, SUM(updown_flag) OVER (PARTITION BY CODE ORDER BY CODE, DT) AS no_up_days\n        FROM (\n            SELECT CODE, DT, \n                CASE WHEN CL > LAG(CL) OVER (PARTITION BY CODE ORDER BY CODE, DT)  THEN 0\n                ELSE 1 END AS updown_flag\n            FROM stock\n        )\n    )\n    GROUP BY CODE, no_up_days\n)\nGROUP BY CODE\n\nAnd to perform the commonly seen funnel analysis on user behavior data in e-commerce industries:\nWITH e1 AS (\n\tSELECT uid,1 AS step1, MIN(etime) AS t1\n\tFROM events\n\tWHERE etime>=end_date-14 AND etime<end_date AND etype='etype1'\n\tGROUP BY uid),\ne2 AS (\n\tSELECT uid,1 AS step2, MIN(e1.t1) as t1, MIN(e2.etime) AS t2\n\tFROM events AS e2 JOIN e1 ON e2.uid = e1.uid\n\tWHERE e2.etime>=end_date-14 AND e2.etime<end_date AND e2.etime>t1 AND e2.etime<t1+7 AND etype='etype2'\n\tGROUP BY uid),\ne3 as (\n\tSELECT uid,1 AS step3, MIN(e2.t1) as t1, MIN(e3.etime) AS t3\n\tFROM events AS e3 JOIN e2 ON e3.uid = e2.uid\n\tWHERE e3.etime>=end_date-14 AND e3.etime<end_date AND e3.etime>t2 AND e3.etime<t1+7 AND etype='etype3'\n\tGROUP BY uid)\nSELECT SUM(step1) AS step1, SUM(step2) AS step2, SUM(step3) AS step3\nFROM e1 LEFT JOIN e2 ON e1.uid = e2.uid LEFT JOIN e3 ON e2.uid = e3.uid\n\nBoth examples involve multilayer nested subqueries, which are difficult to understand and more difficult to write.\nThere are many similar computing tasks in real-world business scenarios. For example:\n\n\nFind players who score three times continuously in one minute;\n\n\nFind the number of users in 7 days who are active in three continuous days;\n\n\nCompute retention rate of every day’s new users a business retains in the next day;\n\n\nCompute the growth rate of a stock on the date when the price is both greater than both the previous and the next five days;\n\n\n…\n\n\nThese complex requirements often necessitate multi-step procedures and involve order-based operations. Their SQL implementations are extremely roundabout with a nearly one hundred lines of N-layered nested statement. Data scientists are wasting their lives in writing such SQL statements.\nSQL is difficult to debug\nComplicated SQL statements are very inconvenient to debug, such as the above commonly seen complex nested SQL statements. To debug them, we need to disassemble the statement and perform debugging layer by layer. Thedisassembly process involves modification of the SQL statement, making the whole debugging procedure very complicated.\n\nThis is because SQL does not have common debugging methods such as “set breakpoint” and “step over”. And data scientists have to go to the trouble of disassembling statements in order to perform debugging, which is a waste of their lives.\nSQL has low performance\nThe query performance of SQL heavily depends on database optimizer. A well-designed database product is able to automatically adopt a more efficient algorithm (instead of executing the SQL statement literally), but often the optimization mechanism breaks down when facing complex computing logics.\nHere is a simple example. To fetch the top 10 from 100 million records, SQL has the following code:\nSELECT TOP 10 x FROM T ORDER BY x DESC\n\nThough this SQL statement contains the ORDER BY keyword, database optimizer will use a more efficient algorithm instead of performing the full sorting (because big data sorting is very slow).\nNow let’s modify the task to find the top 10 from each group, and SQL has the following implementation:\nSELECT * FROM (\n SELECT *, ROW_NUMBER() OVER (PARTITION BY Area ORDER BY Amount DESC) rn\n FROM Orders )\nWHERE rn<=10\n\nThe implementation only gets a little complicated, but most database optimizers already become incapable of performing the optimization. They cannot guess the SQL statement’s purpose but can only execute the literal logic written in the statement to perform the sorting (as there is ORDER BY keyword). As a result, performance is sharply decreased.\nThe SQL statements in real-world business scenarios are far more complicated than that in the example. It is rather common that database optimizer becomes ineffective, such as the above SQL statement handling funnel analysis. The statement involves repeated joins, which are difficult to write and extremely slow to execute.\nLow performance means waiting. For certain big data processing cases, waiting times are from a number of hours to even one day. And during the long waits data scientists’ lives passed.\nSQL is closed\nSQL is the formal language used by databases. The closed databases make data processing difficult. Being closed here refers to the database requirement that data to be computed and processed by the database should be loaded into it in advance. The border between internal data and external data is clear.\nIn real-life businesses, data analysts often need to process data coming from the other sources, including text, Excel, application interface, web crawler, to name a few. Sometimes data coming from any of those sources is only temporarily used, but loading them into the database for each use consumes database space resources and the ETL process is time-consuming. And databases usually have constraints, and non-standardized data cannot be loaded into. This requires to first re-organize data, which needs both time and resources, and then write them to the database, which is also time-consuming (as database writes are very slow). It is during handling these peripheral data organization, loading and retrieval operations that the life of a data scientist is wasted.\nPython is also wasting lives of data scientists\nAs SQL has too many problems, data scientists seek its replacements, and Python is one of them.\nPython surpasses SQL in many aspects. It is easier to debug and more open, and supports procedural computation. However, Python also has its flaws.\nComplex computations are still hard to handle\nPandas, one of Python’s third-party libraries, offers a rich collection of computing functions, which make certain computations simpler than their SQL counterparts. However, handling complex scenarios may still be challenging. For example, here is the code for finding the longest consecutive rising days for each stock mentioned earlier:\nimport pandas as pd\nstock_file = \"StockRecords.txt\"\nstock_info = pd.read_csv(stock_file,sep=\"\\t\")\nstock_info.sort_values(by=['CODE','DT'],inplace=True)\nstock_group = stock_info.groupby(by='CODE')\nstock_info['label'] = stock_info.groupby('CODE')['CL'].diff().fillna(0).le(0).astype(int).cumsum()\nmax_increase_days = {}\nfor code, group in stock_info.groupby('CODE'):\n\tmax_increase_days[code] = group.groupby('label').size().max() – 1\nmax_rise_df = pd.DataFrame(list(max_increase_days.items()), columns=['CODE', 'max_increase_days'])\n\nThe Python code, which involves hard-coding with “for” loop, is still cumbersome. This continues to waste lives of data scientists.\nInconvenient debugging functionalities\nPython has many IDEs and provides much-better-than-SQL debugging capabilities such as “breakpoint”, which makes it unnecessary to disassemble the code.\nBut viewing intermediate values still mainly relies on the print method, which needs to be removed after debugging. This proves to be somewhat cumbersome.\n\nIt takes longer to debug when the corresponding functionality is inconvenient to use, which results in a waste of time for data scientists.\nLow big data processing capability and performance\nPython almost does not have any big data processing capabilities. While the Pandas library can perform in-memory computations like sorting and filtering directly, it struggles with datasets larger than the available memory. In that case data needs to be segmented and processed segment by segment with hardcoding, and the code becomes very complicated.\nPython's parallelism is superficial. To harness multiple CPUs, complex multi-process parallelism is often required. But this is beyond the reach of most data scientists. Bing unable to code parallel processing, data scientists can only perform the slow serial computations and witness their time wasted.\nBoth SQL and Python are not satisfactory enough, then what can truly rescue data scientists?\nesProc SPL – the rescuer of data scientists\nesProc SPL! A very tool specifically designed for structured data processing.\nSPL is characterized by conciseness, ease of understanding and convenient debugging. It supports large-scale data processing and delivers high performance, fundamentally addressing limitations of SQL and Python.\nSimple to write\nSPL offers rich data types and computing class libraries and supports procedural computation, greatly simplifying the code of implementing complex computations. To find the longest consecutive rising days for each stock, for example, SPL has the following code:\n\nThe SPL code is much shorter and does not involve the loop statement. Both data read and write are not difficult.\nTo perform the ecommerce funnel analysis:\n\nCompared with its SQL counterpart, the SPL code is more concise and versatile, and more conforms to the natural way of thinking. It can be used to handle the funnel analysis involving any number of steps. E-commerce funnel analysis with SPL is simpler and aligns more with natural thinking. This code can handle any-step funnel, offering greater simplicity and versatility compared to SQL.\nConvenient to debug\nSPL also offers comprehensive debugging capabilities, including “Set breakpoint”, “Run to cursor”, “Step over”, etc. The result of each step is displayed in real-time on the right side of the IDE. This is convenient as users do not need to split away each subquery or perform manual print any more.\n\nSupport for big data processing\nSPL supports big data processing, no matter whether the data can fit into the memory or not.\nIn-memory computation:\n\nExternal memory computation:\n\nWe can see that the SPL code for the external memory computation is the same as that for the in-memory computation, requiring no additional workload.\nIt is easy and convenient to implement parallel processing in SPL. You just need to add an @m option to the serial computation code.\n\nHigh performance\nIt is easy to write code with low amount of computational work and achieves faster execution in SPL. Take the previously mentioned topN problem as an example, SPL treats topN as a kind of aggregation, eliminating full sorting from the computing logic and achieving much faster speed. This SPL capability is inherent and does not rely on the assistance of optimizer.\nGet TopN from the whole set:\nOrders.groups(;top(10;-Amount))\n\nGet TopN from each group:\n Orders.groups(Area;top(10;-Amount))\n\nThe code of getting topN from each group and that from the entire set is basically the same. Both are straightforward and runs fast.\nSPL offers a wide range of high-performance algorithms, including:\n\n\nFor search: Binary search, sequence-number-based location, index-based search, batch search, etc.\n\n\nFor traversal: Cursor filtering, multipurpose traversal, multicursor, new understanding of aggregation, order-based grouping, application cursor, column-wise computing, etc.\n\n\nFor association: Foreign-key-based pre-association, foreign key numberization, alignment sequence, big dimension table search, one-sided heaping, order-based merge, association-based location, etc.\n\n\nFor cluster computing: cluster composite table, duplicate dimension table, dimension table segmentation, load balancing, etc.\n\n\nEquipped with these algorithms, SPL can achieve a dramatic skyrocketing in computational performance. Data scientists do not need to waste their lives in long waiting.\nOpen system\nSPL is naturally open. It can directly compute any data sources as long as it can access them, including data files such as CSV and Excel, various relational and non-relational databases, and multi-layer data such as JSON and XML, and thus can perform mixed computations.\n\nWith the open system, data scientists can process data coming from various sources directly and efficiently, getting rid of the time spent in data organization and data import/export and thus increasing data processing efficiency.\nHigh portability and enterprise-friendly\nSPL also offers the proprietary file format that is high-performance and portable.\n\nIn contrast, Python lacks a proprietary storage solution. Text files are slow, and databases result in the loss of portability.\nSPL is enterprise-friendly because it is developed in pure Java. After data exploration and analysis, data scientists can integrate SPL with the application by embedding its jars into the latter, facilitating smooth transition from outside to inside.\nWith the convenient to use, easy to debug, high-performance and integration-friendly SPL, data scientists can be freed from the heavy coding work and devote more time to their own businesses.\nesProc SPL is an open-source tool, and its source code is available here.",
    "url": "https://www.indiehackers.com/post/freeing-data-scientists-from-heavy-coding-a-new-era-of-automation-2c2bbe9280",
    "crawl_time": "2025-02-10 06:42:35",
    "translated_content": "SQL is wasting lives of data scientists\nSQL is difficult to write\nNearly all data scientists use SQL for data exploration and analysis. SQL appears deceptively simple and offers a certain degree of interactivity, making it a seemingly ideal choice for these purposes.\nTo perform filtering and grouping operations, for example, SQL just needs one line of code:\nselect id,name from T where id=1\nselect area,sum(amount) from T group by area\n\nBut this is only limited to simple cases. When the computation becomes complex, SQL code becomes complicated, too. For example, to count the longest consecutive rising days of each stock, the SQL statement is as follows:\nSELECT CODE, MAX(con_rise) AS longest_up_days\nFROM (\n    SELECT CODE, COUNT(*) AS con_rise\n    FROM (\n        SELECT CODE, DT, SUM(updown_flag) OVER (PARTITION BY CODE ORDER BY CODE, DT) AS no_up_days\n        FROM (\n            SELECT CODE, DT, \n                CASE WHEN CL > LAG(CL) OVER (PARTITION BY CODE ORDER BY CODE, DT)  THEN 0\n                ELSE 1 END AS updown_flag\n            FROM stock\n        )\n    )\n    GROUP BY CODE, no_up_days\n)\nGROUP BY CODE\n\nAnd to perform the commonly seen funnel analysis on user behavior data in e-commerce industries:\nWITH e1 AS (\n\tSELECT uid,1 AS step1, MIN(etime) AS t1\n\tFROM events\n\tWHERE etime>=end_date-14 AND etime<end_date AND etype='etype1'\n\tGROUP BY uid),\ne2 AS (\n\tSELECT uid,1 AS step2, MIN(e1.t1) as t1, MIN(e2.etime) AS t2\n\tFROM events AS e2 JOIN e1 ON e2.uid = e1.uid\n\tWHERE e2.etime>=end_date-14 AND e2.etime<end_date AND e2.etime>t1 AND e2.etime<t1+7 AND etype='etype2'\n\tGROUP BY uid),\ne3 as (\n\tSELECT uid,1 AS step3, MIN(e2.t1) as t1, MIN(e3.etime) AS t3\n\tFROM events AS e3 JOIN e2 ON e3.uid = e2.uid\n\tWHERE e3.etime>=end_date-14 AND e3.etime<end_date AND e3.etime>t2 AND e3.etime<t1+7 AND etype='etype3'\n\tGROUP BY uid)\nSELECT SUM(step1) AS step1, SUM(step2) AS step2, SUM(step3) AS step3\nFROM e1 LEFT JOIN e2 ON e1.uid = e2.uid LEFT JOIN e3 ON e2.uid = e3.uid\n\nBoth examples involve multilayer nested subqueries, which are difficult to understand and more difficult to write.\nThere are many similar computing tasks in real-world business scenarios. For example:\n\n\nFind players who score three times continuously in one minute;\n\n\nFind the number of users in 7 days who are active in three continuous days;\n\n\nCompute retention rate of every day’s new users a business retains in the next day;\n\n\nCompute the growth rate of a stock on the date when the price is both greater than both the previous and the next five days;\n\n\n…\n\n\nThese complex requirements often necessitate multi-step procedures and involve order-based operations. Their SQL implementations are extremely roundabout with a nearly one hundred lines of N-layered nested statement. Data scientists are wasting their lives in writing such SQL statements.\nSQL is difficult to debug\nComplicated SQL statements are very inconvenient to debug, such as the above commonly seen complex nested SQL statements. To debug them, we need to disassemble the statement and perform debugging layer by layer. Thedisassembly process involves modification of the SQL statement, making the whole debugging procedure very complicated.\n\nThis is because SQL does not have common debugging methods such as “set breakpoint” and “step over”. And data scientists have to go to the trouble of disassembling statements in order to perform debugging, which is a waste of their lives.\nSQL has low performance\nThe query performance of SQL heavily depends on database optimizer. A well-designed database product is able to automatically adopt a more efficient algorithm (instead of executing the SQL statement literally), but often the optimization mechanism breaks down when facing complex computing logics.\nHere is a simple example. To fetch the top 10 from 100 million records, SQL has the following code:\nSELECT TOP 10 x FROM T ORDER BY x DESC\n\nThough this SQL statement contains the ORDER BY keyword, database optimizer will use a more efficient algorithm instead of performing the full sorting (because big data sorting is very slow).\nNow let’s modify the task to find the top 10 from each group, and SQL has the following implementation:\nSELECT * FROM (\n SELECT *, ROW_NUMBER() OVER (PARTITION BY Area ORDER BY Amount DESC) rn\n FROM Orders )\nWHERE rn<=10\n\nThe implementation only gets a little complicated, but most database optimizers already become incapable of performing the optimization. They cannot guess the SQL statement’s purpose but can only execute the literal logic written in the statement to perform the sorting (as there is ORDER BY keyword). As a result, performance is sharply decreased.\nThe SQL statements in real-world business scenarios are far more complicated than that in the example. It is rather common that database optimizer becomes ineffective, such as the above SQL statement handling funnel analysis. The statement involves repeated joins, which are difficult to write and extremely slow to execute.\nLow performance means waiting. For certain big data processing cases, waiting times are from a number of hours to even one day. And during the long waits data scientists’ lives passed.\nSQL is closed\nSQL is the formal language used by databases. The closed databases make data processing difficult. Being closed here refers to the database requirement that data to be computed and processed by the database should be loaded into it in advance. The border between internal data and external data is clear.\nIn real-life businesses, data analysts often need to process data coming from the other sources, including text, Excel, application interface, web crawler, to name a few. Sometimes data coming from any of those sources is only temporarily used, but loading them into the database for each use consumes database space resources and the ETL process is time-consuming. And databases usually have constraints, and non-standardized data cannot be loaded into. This requires to first re-organize data, which needs both time and resources, and then write them to the database, which is also time-consuming (as database writes are very slow). It is during handling these peripheral data organization, loading and retrieval operations that the life of a data scientist is wasted.\nPython is also wasting lives of data scientists\nAs SQL has too many problems, data scientists seek its replacements, and Python is one of them.\nPython surpasses SQL in many aspects. It is easier to debug and more open, and supports procedural computation. However, Python also has its flaws.\nComplex computations are still hard to handle\nPandas, one of Python’s third-party libraries, offers a rich collection of computing functions, which make certain computations simpler than their SQL counterparts. However, handling complex scenarios may still be challenging. For example, here is the code for finding the longest consecutive rising days for each stock mentioned earlier:\nimport pandas as pd\nstock_file = \"StockRecords.txt\"\nstock_info = pd.read_csv(stock_file,sep=\"\\t\")\nstock_info.sort_values(by=['CODE','DT'],inplace=True)\nstock_group = stock_info.groupby(by='CODE')\nstock_info['label'] = stock_info.groupby('CODE')['CL'].diff().fillna(0).le(0).astype(int).cumsum()\nmax_increase_days = {}\nfor code, group in stock_info.groupby('CODE'):\n\tmax_increase_days[code] = group.groupby('label').size().max() – 1\nmax_rise_df = pd.DataFrame(list(max_increase_days.items()), columns=['CODE', 'max_increase_days'])\n\nThe Python code, which involves hard-coding with “for” loop, is still cumbersome. This continues to waste lives of data scientists.\nInconvenient debugging functionalities\nPython has many IDEs and provides much-better-than-SQL debugging capabilities such as “breakpoint”, which makes it unnecessary to disassemble the code.\nBut viewing intermediate values still mainly relies on the print method, which needs to be removed after debugging. This proves to be somewhat cumbersome.\n\nIt takes longer to debug when the corresponding functionality is inconvenient to use, which results in a waste of time for data scientists.\nLow big data processing capability and performance\nPython almost does not have any big data processing capabilities. While the Pandas library can perform in-memory computations like sorting and filtering directly, it struggles with datasets larger than the available memory. In that case data needs to be segmented and processed segment by segment with hardcoding, and the code becomes very complicated.\nPython's parallelism is superficial. To harness multiple CPUs, complex multi-process parallelism is often required. But this is beyond the reach of most data scientists. Bing unable to code parallel processing, data scientists can only perform the slow serial computations and witness their time wasted.\nBoth SQL and Python are not satisfactory enough, then what can truly rescue data scientists?\nesProc SPL – the rescuer of data scientists\nesProc SPL! A very tool specifically designed for structured data processing.\nSPL is characterized by conciseness, ease of understanding and convenient debugging. It supports large-scale data processing and delivers high performance, fundamentally addressing limitations of SQL and Python.\nSimple to write\nSPL offers rich data types and computing class libraries and supports procedural computation, greatly simplifying the code of implementing complex computations. To find the longest consecutive rising days for each stock, for example, SPL has the following code:\n\nThe SPL code is much shorter and does not involve the loop statement. Both data read and write are not difficult.\nTo perform the ecommerce funnel analysis:\n\nCompared with its SQL counterpart, the SPL code is more concise and versatile, and more conforms to the natural way of thinking. It can be used to handle the funnel analysis involving any number of steps. E-commerce funnel analysis with SPL is simpler and aligns more with natural thinking. This code can handle any-step funnel, offering greater simplicity and versatility compared to SQL.\nConvenient to debug\nSPL also offers comprehensive debugging capabilities, including “Set breakpoint”, “Run to cursor”, “Step over”, etc. The result of each step is displayed in real-time on the right side of the IDE. This is convenient as users do not need to split away each subquery or perform manual print any more.\n\nSupport for big data processing\nSPL supports big data processing, no matter whether the data can fit into the memory or not.\nIn-memory computation:\n\nExternal memory computation:\n\nWe can see that the SPL code for the external memory computation is the same as that for the in-memory computation, requiring no additional workload.\nIt is easy and convenient to implement parallel processing in SPL. You just need to add an @m option to the serial computation code.\n\nHigh performance\nIt is easy to write code with low amount of computational work and achieves faster execution in SPL. Take the previously mentioned topN problem as an example, SPL treats topN as a kind of aggregation, eliminating full sorting from the computing logic and achieving much faster speed. This SPL capability is inherent and does not rely on the assistance of optimizer.\nGet TopN from the whole set:\nOrders.groups(;top(10;-Amount))\n\nGet TopN from each group:\n Orders.groups(Area;top(10;-Amount))\n\nThe code of getting topN from each group and that from the entire set is basically the same. Both are straightforward and runs fast.\nSPL offers a wide range of high-performance algorithms, including:\n\n\nFor search: Binary search, sequence-number-based location, index-based search, batch search, etc.\n\n\nFor traversal: Cursor filtering, multipurpose traversal, multicursor, new understanding of aggregation, order-based grouping, application cursor, column-wise computing, etc.\n\n\nFor association: Foreign-key-based pre-association, foreign key numberization, alignment sequence, big dimension table search, one-sided heaping, order-based merge, association-based location, etc.\n\n\nFor cluster computing: cluster composite table, duplicate dimension table, dimension table segmentation, load balancing, etc.\n\n\nEquipped with these algorithms, SPL can achieve a dramatic skyrocketing in computational performance. Data scientists do not need to waste their lives in long waiting.\nOpen system\nSPL is naturally open. It can directly compute any data sources as long as it can access them, including data files such as CSV and Excel, various relational and non-relational databases, and multi-layer data such as JSON and XML, and thus can perform mixed computations.\n\nWith the open system, data scientists can process data coming from various sources directly and efficiently, getting rid of the time spent in data organization and data import/export and thus increasing data processing efficiency.\nHigh portability and enterprise-friendly\nSPL also offers the proprietary file format that is high-performance and portable.\n\nIn contrast, Python lacks a proprietary storage solution. Text files are slow, and databases result in the loss of portability.\nSPL is enterprise-friendly because it is developed in pure Java. After data exploration and analysis, data scientists can integrate SPL with the application by embedding its jars into the latter, facilitating smooth transition from outside to inside.\nWith the convenient to use, easy to debug, high-performance and integration-friendly SPL, data scientists can be freed from the heavy coding work and devote more time to their own businesses.\nesProc SPL is an open-source tool, and its source code is available here.",
    "category": "其他"
  },
  {
    "title": "How to Network Online: Naturally Find Clients, Develop Partnerships And Make Smart Friends",
    "content": "https://benjaminsnotes.substack.com/p/how-to-network",
    "url": "https://www.indiehackers.com/post/how-to-network-online-naturally-find-clients-develop-partnerships-and-make-smart-friends-d70b49264c",
    "crawl_time": "2025-02-10 06:42:38",
    "translated_content": "https://benjaminsnotes.substack.com/p/how-to-network",
    "category": "其他"
  }
]