[
  {
    "title": "ğŸš€ Built a Tool to Help You Speak Under Pressureâ€”Feedback Wanted!",
    "content": "Practice speaking under pressure with SpeakFast.aiâ€”improve clarity, confidence, and emotional delivery in real-world scenarios.\n\nHey folks,\nIâ€™ve been hacking on this tool for a couple of weeks. Itâ€™s not perfect but it's ready for public feedback.\nğŸš€ Why I Built This\nBeing able to think and articulate clearly under pressure is a skillâ€”one that most of us never practice, even though it shapes how weâ€™re perceived in \"high-stakes\" conversations.\nWeâ€™ve all had those moments:\n\nYou're put on the spot, and your mind blanks.\nYou have a great idea but canâ€™t explain it fast enough.\nYou try to advocate for yourself, but the words donâ€™t come out right.\n\nIf that sounds familiar, this might be useful to you.\nğŸ™ What I Built\nSpeakFast.ai is a tool to help you practice thinking and speaking under pressure through AI-driven simulated conversations.\n\nPick a preset scene (e.g., job interviews, salary negotiations, investor pitches).\nStart a live conversation with AI.\nGet instant feedback on clarity, confidence, and perception.\n\nIâ€™m giving away 30 minutes of free accessâ€”just go to SpeakFast.ai and start a session.\nğŸ¤” Why Not Just Use ChatGPT?  (or other similar tools)\nI love ChatGPTâ€™s voice mode and it's possible to prompt it to do something like this, but itâ€™s missing a few things:\n\nNo feedback on tone, confidence, or clarity.\nToo easy to misleadâ€”it only focuses on what you say, not how you say it.\nNot designed for real-world pressureâ€”it answers questions, gives solid advice, but it wasn't designed for this niche use case.\n\nI've tried other tools as well, but I did not find one I enjoyed spending time on.\nğŸ” What I Need From You\nIâ€™m still figuring out exactly who this is for and what you need. Early feedback from family and friends shaped the initial experience, but Iâ€™m open to major changes.\nIf this sounds remotely useful to you, Iâ€™d love for you to try it and tell me whatâ€™s missing.\n\nPS: Product is still early, so expect some bugs. Let me know if anything breaks!\n\nğŸ‘‰ Try SpeakFast.ai now!\nWould appreciate any thoughtsâ€”positive or critical. Thanks!",
    "url": "https://www.indiehackers.com/post/built-a-tool-to-help-you-speak-under-pressure-feedback-wanted-12e9422c44",
    "crawl_time": "2025-02-10 06:42:08",
    "translated_content": "ç»ƒä¹ åœ¨Speakfast.aiä¸‹åœ¨å‹åŠ›ä¸‹è¯´è¯ - åœ¨ç°å®ä¸–ç•Œä¸­çš„æ¸…æ™°åº¦ï¼Œä¿¡å¿ƒå’Œæƒ…æ„Ÿäº¤ä»˜ã€‚\n\nå˜¿ï¼Œä¼™è®¡ä»¬ï¼Œ\næˆ‘å·²ç»åœ¨è¿™ä¸ªå·¥å…·ä¸Šé»‘å®¢äº†å‡ å‘¨ã€‚å®ƒå¹¶ä¸å®Œç¾ï¼Œä½†å·²å‡†å¤‡å¥½è·å¾—å…¬ä¼—åé¦ˆã€‚\nğŸš€ä¸ºä»€ä¹ˆæˆ‘å»ºé€ è¿™ä¸ª\nèƒ½å¤Ÿåœ¨å‹åŠ›ä¸‹æ¸…æ¥šåœ°æ€è€ƒå’Œæ¸…æ¥šåœ°è¡¨è¾¾çš„æ˜¯ä¸€é¡¹æŠ€èƒ½ï¼Œå³ä½¿æˆ‘ä»¬å¤§å¤šæ•°äººéƒ½ä»æœªç»ƒä¹ è¿‡ï¼Œå³ä½¿å®ƒå¡‘é€ äº†æˆ‘ä»¬åœ¨â€œé«˜é£é™©â€å¯¹è¯ä¸­çš„çœ‹æ³•ã€‚\næˆ‘ä»¬éƒ½æœ‰é‚£äº›æ—¶åˆ»ï¼š\n\næ‚¨å½“åœºï¼Œå¤´è„‘ç©ºç™½ã€‚\næ‚¨æœ‰ä¸€ä¸ªå¥½ä¸»æ„ï¼Œä½†ä¸èƒ½è¶³å¤Ÿå¿«åœ°è§£é‡Šå®ƒã€‚\næ‚¨å°è¯•è‡ªå·±æå€¡ï¼Œä½†è¯è¯­ä¸æ˜¯æ­£ç¡®çš„ã€‚\n\nå¦‚æœå¬èµ·æ¥å¾ˆç†Ÿæ‚‰ï¼Œè¿™å¯¹æ‚¨å¯èƒ½å¾ˆæœ‰ç”¨ã€‚\nğŸ™æˆ‘å»ºé€ çš„\nSpeakFast.AIæ˜¯ä¸€ç§å·¥å…·ï¼Œå¯ä»¥é€šè¿‡AIé©±åŠ¨çš„æ¨¡æ‹Ÿå¯¹è¯æ¥å¸®åŠ©æ‚¨åœ¨å‹åŠ›ä¸‹è¿›è¡Œæ€è€ƒå’Œè®²è¯ã€‚\n\né€‰æ‹©ä¸€ä¸ªé¢„è®¾åœºæ™¯ï¼ˆä¾‹å¦‚ï¼Œæ±‚èŒé¢è¯•ï¼Œå·¥èµ„è°ˆåˆ¤ï¼ŒæŠ•èµ„è€…çš„æ¨é”€ï¼‰ã€‚\nä¸AIè¿›è¡Œç°åœºå¯¹è¯ã€‚\nè·å¾—æœ‰å…³æ¸…æ™°åº¦ï¼Œä¿¡å¿ƒå’Œæ„ŸçŸ¥çš„å³æ—¶åé¦ˆã€‚\n\næˆ‘å°†æä¾›30åˆ†é’Ÿçš„å…è´¹è®¿é—®æƒé™ - åªéœ€è®¿é—®Speakfast.aiå¹¶å¼€å§‹ä¼šè®®ã€‚\nğŸ¤”ä¸ºä»€ä¹ˆä¸ä½¿ç”¨chatgptï¼Ÿ  ï¼ˆæˆ–å…¶ä»–ç±»ä¼¼å·¥å…·ï¼‰\næˆ‘å–œæ¬¢Chatgptçš„è¯­éŸ³æ¨¡å¼ï¼Œå¯ä»¥æç¤ºå®ƒåšç±»ä¼¼çš„äº‹æƒ…ï¼Œä½†æ˜¯å®ƒç¼ºå°‘ä¸€äº›äº‹æƒ…ï¼š\n\næ²¡æœ‰å…³äºè¯­æ°”ï¼Œè‡ªä¿¡æˆ–æ¸…æ™°åº¦çš„åé¦ˆã€‚\nå¤ªå®¹æ˜“è¯¯å¯¼äº† - å®ƒåªä¸“æ³¨äºæ‚¨æ‰€è¯´çš„è¯ï¼Œè€Œä¸æ˜¯æ‚¨è¯´çš„è¯ã€‚\nä¸æ˜¯ä¸ºç°å®ä¸–ç•Œçš„å‹åŠ›è€Œè®¾è®¡çš„ï¼Œå®ƒå›ç­”é—®é¢˜ï¼Œæä¾›äº†å¯é çš„å»ºè®®ï¼Œä½†ä¸æ˜¯ä¸ºè¿™ç§åˆ©åŸºç”¨ä¾‹è€Œè®¾è®¡çš„ã€‚\n\næˆ‘ä¹Ÿå°è¯•è¿‡å…¶ä»–å·¥å…·ï¼Œä½†æ˜¯æˆ‘æ²¡æœ‰æ‰¾åˆ°æˆ‘å–œæ¬¢èŠ±æ—¶é—´çš„å·¥å…·ã€‚\nğŸ”æˆ‘éœ€è¦ä½ çš„ä¸œè¥¿\næˆ‘ä»åœ¨ç¡®åˆ‡åœ°å¼„æ¸…æ¥šè¿™æ˜¯è°ä»¥åŠæ‚¨éœ€è¦ä»€ä¹ˆã€‚å®¶äººå’Œæœ‹å‹çš„æ—©æœŸåé¦ˆå¡‘é€ äº†æœ€åˆçš„ç»å†ï¼Œä½†æˆ‘æ„¿æ„è¿›è¡Œé‡å¤§å˜åŒ–ã€‚\nå¦‚æœè¿™å¬èµ·æ¥å¯¹æ‚¨æœ‰ç”¨ï¼Œæˆ‘å¸Œæœ›æ‚¨èƒ½å°è¯•ä¸€ä¸‹ï¼Œå¹¶å‘Šè¯‰æˆ‘ç¼ºå°‘ä»€ä¹ˆã€‚\n\nPSï¼šäº§å“è¿˜å¾ˆæ—©ï¼Œå› æ­¤å¯ä»¥æœŸå¾…ä¸€äº›é”™è¯¯ã€‚è®©æˆ‘çŸ¥é“æ˜¯å¦æœ‰ä»»ä½•ç ´è£‚ï¼\n\nğŸ‘‰ç°åœ¨å°è¯•ä½¿ç”¨Speakfast.aiï¼\nä¼šæ¬£èµä»»ä½•æƒ³æ³• - é˜³æ€§æˆ–å…³é”®ã€‚è°¢è°¢ï¼",
    "category": "äº§å“"
  },
  {
    "title": "Should I launch the landing page on product hunt and not a working MVP?",
    "content": "Hi guys,\nI have the landing page (which looks beautiful) with a lead form and waiting list but not a working product. Should I launch it like that and hope for waiting list enrollments and feedback on the idea or wait to finish the MVP and launch then? Or both? Thanks a lot!",
    "url": "https://www.indiehackers.com/post/should-i-launch-the-landing-page-on-product-hunt-and-not-a-working-mvp-6bc3fd6973",
    "crawl_time": "2025-02-10 06:42:14",
    "translated_content": "Hi guys,\nI have the landing page (which looks beautiful) with a lead form and waiting list but not a working product. Should I launch it like that and hope for waiting list enrollments and feedback on the idea or wait to finish the MVP and launch then? Or both? Thanks a lot!",
    "category": "å…¶ä»–"
  },
  {
    "title": "Data Analysis Showdown: Comparing SQL, Python, and esProc SPL",
    "content": "Talk is cheap; letâ€™s show the codes.\n1. User Session Count\nUser behavior data table\n\nA session is considered over if a user does not take any action within 10 minutes, or if they do not log in within 5 minutes after logging out. Calculate the number of sessions for each user.\nSPL\n\nSQL\nWITH login_data AS (\n    SELECT userid, action_type, action_time,\n        LAG(action_time) OVER (PARTITION BY userid ORDER BY action_time) AS prev_time,\n        LAG(action_type) OVER (PARTITION BY userid ORDER BY action_time) AS prev_action\n    FROM session_data)\nSELECT userid, COUNT(*) AS session_count\nFROM (\n    SELECT userid, action_type, action_time, prev_time, prev_action,\n        CASE\n            WHEN prev_time IS NULL OR (action_time - prev_time) > 60 \n                OR (prev_action = 'exit' AND (action_time - prev_time) > 300 )\n            THEN 1\n            ELSE 0\n        END AS is_new_session\n    FROM login_data)\nWHERE is_new_session = 1\nGROUP BY userid;\n\nPython\nlogin_data = pd.read_csv(\"session_data.csv\")\nlogin_data['action_time'] = pd.to_datetime(login_data['action_time'])\ngrouped = login_data.groupby(\"userid\")\nsession_count = {}\nfor uid, sub_df in grouped:\n    session_count[uid] = 0\n    start_index = 0\n    for i in range(1, len(sub_df)):\n        current = sub_df.iloc[i]\n        last = sub_df.iloc[start_index]\n        last_action = last['action_type']\n        if (current[\"action_time\"] - last[\"action_time\"]).seconds > 600 or \\\n            (last_action==\"exit\" and (current[\"action_time\"] - last[\"action_time\"]).seconds > 300):\n            session_count[uid] += 1\n        start_index = i\n    session_count[uid] += 1\nsession_cnt = pd.DataFrame(list(session_count.items()), columns=['UID', 'session_count'])\n\n2. Count the players who score 3 times in a row within 1 minute\nScore table of a ball game\n\nSPL\n\nSQL\nWITH numbered_scores AS (\n    SELECT team, player, play_time, score,\n        ROW_NUMBER() OVER (ORDER BY play_time) AS rn\n    FROM ball_game)\nSELECT DISTINCT s1.player\nFROM numbered_scores s1\n    JOIN numbered_scores s2 ON s1.player = s2.player AND s1.rn = s2.rn - 1\n    JOIN numbered_scores s3 ON s1.player = s3.player AND s1.rn = s3.rn - 2\nWHERE (s3.play_time - s1.play_time) <60 ;\n\nPython\ndf = pd.read_csv(\"ball_game.csv\")\ndf[\"play_time\"] = pd.to_datetime(df[\"play_time\"])\nresult_players = []\nplayer = None\nstart_index = 0\nconsecutive_scores = 0\nfor i in range(len(df)-2):\n    current = df.iloc[i]\n    if player != current[\"player\"]:\n        player = current[\"player\"]\n        consecutive_scores = 1\n    else:\n        consecutive_scores += 1\n    last2 = df.iloc[i-2] if i >=2 else None\n    if consecutive_scores >= 3 and (current['play_time'] - last2['play_time']).seconds < 60:\n        result_players.append(player)\nresult_players = list(set(result_players))\n\n3. Calculate the number of users who are active for three consecutive days within every 7 days\nUser login data table\n\nSPL\n\nSQL\nWITH all_dates AS (\n    SELECT DISTINCT TRUNC(ts) AS login_date\n    FROM login_data),\nuser_login_counts AS (\n    SELECT userid, TRUNC(ts) AS login_date, \n        (CASE WHEN COUNT(*)>=1 THEN 1 ELSE 0 END) AS login_count\n    FROM login_data\n    GROUP BY userid, TRUNC(ts)),\nwhether_login AS (\n    SELECT u.userid, ad.login_date, NVL(ulc.login_count, 0) AS login_count\n    FROM all_dates ad\n    CROSS JOIN (\n        SELECT DISTINCT userid\n        FROM login_data) u\n    LEFT JOIN user_login_counts ulc\n    ON u.userid = ulc.userid\n    AND ad.login_date = ulc.login_date\n    ORDER BY u.userid, ad.login_date),\nwhether_login_rn AS (\n    SELECT userid,login_date,login_count,ROWNUM AS rn \n    FROM whether_login),\nwhether_eq AS(\n    SELECT userid,login_date,login_count,rn,\n        (CASE \n            WHEN LAG(login_count,1) OVER (ORDER BY rn)= login_count \n                AND login_count =1 AND LAG(userid,1) OVER (ORDER BY rn)=userid \n            THEN 0 \n            ELSE 1 \n        END) AS wether_e \n    FROM whether_login_rn\n),\nnumbered_sequence AS (\n    SELECT userid,login_date,login_count,rn, wether_e,\n        SUM(wether_e) OVER (ORDER BY rn) AS lab\n    FROM whether_eq),\nconsecutive_logins_num AS (\n    SELECT userid,login_date,login_count,rn, wether_e,lab,\n        (SELECT (CASE WHEN max(COUNT(*))<3 THEN 0 ELSE 1 END)\n        FROM numbered_sequence b\n        WHERE b.rn BETWEEN a.rn - 6 AND a.rn\n        AND b.userid=a.userid\n        GROUP BY b. lab) AS cnt\n    FROM numbered_sequence a)\nSELECT login_date,SUM(cnt) AS cont3_num\nFROM consecutive_logins_num\nWHERE login_date>=(SELECT MIN(login_date) FROM all_dates)+6\nGROUP BY login_date\nORDER BY login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngrouped = df.groupby(\"userid\")\naligned_dates = pd.date_range(start=df[\"ts\"].min(), end=df[\"ts\"].max(), freq='D')\nuser_date_wether_con3days = []\nfor uid, group in grouped:\n    group = group.drop_duplicates('ts')\n    aligned_group = group.set_index(\"ts\").reindex(aligned_dates)\n    consecutive_logins = aligned_group.rolling(window=7)\n    n = 0\n    date_wether_con3days = []\n    for r in consecutive_logins:\n        n += 1\n        if n<7:\n            continue\n        else:\n            ds = r['userid'].isna().cumsum()\n            cont_login_times = r.groupby(ds).userid.count().max()\n            wether_cont3days = 1 if cont_login_times>=3 else 0\n            date_wether_con3days.append(wether_cont3days)\n    user_date_wether_con3days.append(date_wether_con3days)\narr = np.array(user_date_wether_con3days)\nday7_cont3num = np.sum(arr,axis=0)\nresult = pd.DataFrame({'dt':aligned_dates[6:],'cont3_num':day7_cont3num})\n\n4. Calculate the next-day retention rate of new users per day\nUser login data table\n\nSPL\n\nA2: Group by user; record the first login date and check whether the user logs in the next day.\nA3: Calculate the next-day retention rate based on the login date of the next day.\nSQL\nWITH first_login AS (\n    SELECT userid, MIN(TRUNC(ts)) AS first_login_date\n    FROM login_data\n    GROUP BY userid),\nnext_day_login AS (\n    SELECT DISTINCT(fl.userid), fl.first_login_date, TRUNC(ld.ts) AS next_day_login_date\n    FROM first_login fl\n    LEFT JOIN login_data ld ON fl.userid = ld.userid\n    WHERE TRUNC(ld.ts) = fl.first_login_date + 1),\nday_new_users AS(\n    SELECT first_login_date,COUNT(*) AS new_user_num\n    FROM first_login\n    GROUP BY first_login_date),\nnext_new_users AS(\n    SELECT next_day_login_date, COUNT(*) AS next_user_num\n    FROM next_day_login\n    GROUP BY next_day_login_date),\nall_date AS(\n    SELECT DISTINCT(TRUNC(ts)) AS login_date\n    FROM login_data)\nSELECT all_date.login_date+1 AS dt,dn. new_user_num,nn. next_user_num,\n    (CASE \n        WHEN nn. next_day_login_date IS NULL \n        THEN 0 \n        ELSE nn.next_user_num \n    END)/dn.new_user_num AS ret_rate\nFROM all_date\n    JOIN day_new_users dn ON all_date.login_date=dn.first_login_date\n    LEFT JOIN next_new_users nn ON dn.first_login_date+1=nn. next_day_login_date\nORDER BY all_date.login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngp = df.groupby('userid')\nrow = []\nfor uid,g in gp:\n    fst_dt = g.iloc[0].ts\n    sec_dt = fst_dt + pd.Timedelta(days=1)\n    all_dt = g.ts.values\n    wether_sec_login = sec_dt in all_dt\n    row.append([uid,fst_dt,sec_dt,wether_sec_login])\nuser_wether_ret_df = pd.DataFrame(row,columns=['userid','fst_dt','sec_dt','wether_sec_login'])\nresult = user_wether_ret_df.groupby('sec_dt').apply(lambda x:x['wether_sec_login'].sum()/len(x))\n\n5. Calculate the increase of stock price on the day when it is higher than those on the previous and next 5 days\nStock price data table\n\nSPL\n\nA2: The position where the stock price is higher than those of the previous and next 5 days.\nA3: Calculate the increase at that time.\nSQL\nSELECT closing/closing_pre-1 AS raise\nFROM(\n    SELECT dt, closing, ROWNUM AS rn,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING) AS max_pre,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 1 FOLLOWING AND 5 FOLLOWING) AS max_suf,\n        LAG(closing,1) OVER (ORDER BY dt) AS closing_pre\n    FROM stock)\nWHERE rn>5 AND rn<=(select count(*) FROM stock)-5\n    AND CLOSING>max_pre AND CLOSING>max_suf;\n\nPython\nstock_price_df = pd.read_csv('STOCK.csv')\nprice_increase_list = []\nfor i in range(5, len(stock_price_df)-5):\n    if stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i-5:i]) and \\\n    stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i+1:i+6]):\n        price_increase = stock_price_df['CLOSING'][i] / stock_price_df['CLOSING'][i-1]-1\n        price_increase_list.append(price_increase)\nresult = price_increase_list\n\nesProc SPL is open-source and here's the Open-Source Address.",
    "url": "https://www.indiehackers.com/post/data-analysis-showdown-comparing-sql-python-and-esproc-spl-dd4c31d7ce",
    "crawl_time": "2025-02-10 06:42:24",
    "translated_content": "Talk is cheap; letâ€™s show the codes.\n1. User Session Count\nUser behavior data table\n\nA session is considered over if a user does not take any action within 10 minutes, or if they do not log in within 5 minutes after logging out. Calculate the number of sessions for each user.\nSPL\n\nSQL\nWITH login_data AS (\n    SELECT userid, action_type, action_time,\n        LAG(action_time) OVER (PARTITION BY userid ORDER BY action_time) AS prev_time,\n        LAG(action_type) OVER (PARTITION BY userid ORDER BY action_time) AS prev_action\n    FROM session_data)\nSELECT userid, COUNT(*) AS session_count\nFROM (\n    SELECT userid, action_type, action_time, prev_time, prev_action,\n        CASE\n            WHEN prev_time IS NULL OR (action_time - prev_time) > 60 \n                OR (prev_action = 'exit' AND (action_time - prev_time) > 300 )\n            THEN 1\n            ELSE 0\n        END AS is_new_session\n    FROM login_data)\nWHERE is_new_session = 1\nGROUP BY userid;\n\nPython\nlogin_data = pd.read_csv(\"session_data.csv\")\nlogin_data['action_time'] = pd.to_datetime(login_data['action_time'])\ngrouped = login_data.groupby(\"userid\")\nsession_count = {}\nfor uid, sub_df in grouped:\n    session_count[uid] = 0\n    start_index = 0\n    for i in range(1, len(sub_df)):\n        current = sub_df.iloc[i]\n        last = sub_df.iloc[start_index]\n        last_action = last['action_type']\n        if (current[\"action_time\"] - last[\"action_time\"]).seconds > 600 or \\\n            (last_action==\"exit\" and (current[\"action_time\"] - last[\"action_time\"]).seconds > 300):\n            session_count[uid] += 1\n        start_index = i\n    session_count[uid] += 1\nsession_cnt = pd.DataFrame(list(session_count.items()), columns=['UID', 'session_count'])\n\n2. Count the players who score 3 times in a row within 1 minute\nScore table of a ball game\n\nSPL\n\nSQL\nWITH numbered_scores AS (\n    SELECT team, player, play_time, score,\n        ROW_NUMBER() OVER (ORDER BY play_time) AS rn\n    FROM ball_game)\nSELECT DISTINCT s1.player\nFROM numbered_scores s1\n    JOIN numbered_scores s2 ON s1.player = s2.player AND s1.rn = s2.rn - 1\n    JOIN numbered_scores s3 ON s1.player = s3.player AND s1.rn = s3.rn - 2\nWHERE (s3.play_time - s1.play_time) <60 ;\n\nPython\ndf = pd.read_csv(\"ball_game.csv\")\ndf[\"play_time\"] = pd.to_datetime(df[\"play_time\"])\nresult_players = []\nplayer = None\nstart_index = 0\nconsecutive_scores = 0\nfor i in range(len(df)-2):\n    current = df.iloc[i]\n    if player != current[\"player\"]:\n        player = current[\"player\"]\n        consecutive_scores = 1\n    else:\n        consecutive_scores += 1\n    last2 = df.iloc[i-2] if i >=2 else None\n    if consecutive_scores >= 3 and (current['play_time'] - last2['play_time']).seconds < 60:\n        result_players.append(player)\nresult_players = list(set(result_players))\n\n3. Calculate the number of users who are active for three consecutive days within every 7 days\nUser login data table\n\nSPL\n\nSQL\nWITH all_dates AS (\n    SELECT DISTINCT TRUNC(ts) AS login_date\n    FROM login_data),\nuser_login_counts AS (\n    SELECT userid, TRUNC(ts) AS login_date, \n        (CASE WHEN COUNT(*)>=1 THEN 1 ELSE 0 END) AS login_count\n    FROM login_data\n    GROUP BY userid, TRUNC(ts)),\nwhether_login AS (\n    SELECT u.userid, ad.login_date, NVL(ulc.login_count, 0) AS login_count\n    FROM all_dates ad\n    CROSS JOIN (\n        SELECT DISTINCT userid\n        FROM login_data) u\n    LEFT JOIN user_login_counts ulc\n    ON u.userid = ulc.userid\n    AND ad.login_date = ulc.login_date\n    ORDER BY u.userid, ad.login_date),\nwhether_login_rn AS (\n    SELECT userid,login_date,login_count,ROWNUM AS rn \n    FROM whether_login),\nwhether_eq AS(\n    SELECT userid,login_date,login_count,rn,\n        (CASE \n            WHEN LAG(login_count,1) OVER (ORDER BY rn)= login_count \n                AND login_count =1 AND LAG(userid,1) OVER (ORDER BY rn)=userid \n            THEN 0 \n            ELSE 1 \n        END) AS wether_e \n    FROM whether_login_rn\n),\nnumbered_sequence AS (\n    SELECT userid,login_date,login_count,rn, wether_e,\n        SUM(wether_e) OVER (ORDER BY rn) AS lab\n    FROM whether_eq),\nconsecutive_logins_num AS (\n    SELECT userid,login_date,login_count,rn, wether_e,lab,\n        (SELECT (CASE WHEN max(COUNT(*))<3 THEN 0 ELSE 1 END)\n        FROM numbered_sequence b\n        WHERE b.rn BETWEEN a.rn - 6 AND a.rn\n        AND b.userid=a.userid\n        GROUP BY b. lab) AS cnt\n    FROM numbered_sequence a)\nSELECT login_date,SUM(cnt) AS cont3_num\nFROM consecutive_logins_num\nWHERE login_date>=(SELECT MIN(login_date) FROM all_dates)+6\nGROUP BY login_date\nORDER BY login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngrouped = df.groupby(\"userid\")\naligned_dates = pd.date_range(start=df[\"ts\"].min(), end=df[\"ts\"].max(), freq='D')\nuser_date_wether_con3days = []\nfor uid, group in grouped:\n    group = group.drop_duplicates('ts')\n    aligned_group = group.set_index(\"ts\").reindex(aligned_dates)\n    consecutive_logins = aligned_group.rolling(window=7)\n    n = 0\n    date_wether_con3days = []\n    for r in consecutive_logins:\n        n += 1\n        if n<7:\n            continue\n        else:\n            ds = r['userid'].isna().cumsum()\n            cont_login_times = r.groupby(ds).userid.count().max()\n            wether_cont3days = 1 if cont_login_times>=3 else 0\n            date_wether_con3days.append(wether_cont3days)\n    user_date_wether_con3days.append(date_wether_con3days)\narr = np.array(user_date_wether_con3days)\nday7_cont3num = np.sum(arr,axis=0)\nresult = pd.DataFrame({'dt':aligned_dates[6:],'cont3_num':day7_cont3num})\n\n4. Calculate the next-day retention rate of new users per day\nUser login data table\n\nSPL\n\nA2: Group by user; record the first login date and check whether the user logs in the next day.\nA3: Calculate the next-day retention rate based on the login date of the next day.\nSQL\nWITH first_login AS (\n    SELECT userid, MIN(TRUNC(ts)) AS first_login_date\n    FROM login_data\n    GROUP BY userid),\nnext_day_login AS (\n    SELECT DISTINCT(fl.userid), fl.first_login_date, TRUNC(ld.ts) AS next_day_login_date\n    FROM first_login fl\n    LEFT JOIN login_data ld ON fl.userid = ld.userid\n    WHERE TRUNC(ld.ts) = fl.first_login_date + 1),\nday_new_users AS(\n    SELECT first_login_date,COUNT(*) AS new_user_num\n    FROM first_login\n    GROUP BY first_login_date),\nnext_new_users AS(\n    SELECT next_day_login_date, COUNT(*) AS next_user_num\n    FROM next_day_login\n    GROUP BY next_day_login_date),\nall_date AS(\n    SELECT DISTINCT(TRUNC(ts)) AS login_date\n    FROM login_data)\nSELECT all_date.login_date+1 AS dt,dn. new_user_num,nn. next_user_num,\n    (CASE \n        WHEN nn. next_day_login_date IS NULL \n        THEN 0 \n        ELSE nn.next_user_num \n    END)/dn.new_user_num AS ret_rate\nFROM all_date\n    JOIN day_new_users dn ON all_date.login_date=dn.first_login_date\n    LEFT JOIN next_new_users nn ON dn.first_login_date+1=nn. next_day_login_date\nORDER BY all_date.login_date;\n\nPython\ndf = pd.read_csv(\"login_data.csv\")\ndf[\"ts\"] = pd.to_datetime(df[\"ts\"]).dt.date\ngp = df.groupby('userid')\nrow = []\nfor uid,g in gp:\n    fst_dt = g.iloc[0].ts\n    sec_dt = fst_dt + pd.Timedelta(days=1)\n    all_dt = g.ts.values\n    wether_sec_login = sec_dt in all_dt\n    row.append([uid,fst_dt,sec_dt,wether_sec_login])\nuser_wether_ret_df = pd.DataFrame(row,columns=['userid','fst_dt','sec_dt','wether_sec_login'])\nresult = user_wether_ret_df.groupby('sec_dt').apply(lambda x:x['wether_sec_login'].sum()/len(x))\n\n5. Calculate the increase of stock price on the day when it is higher than those on the previous and next 5 days\nStock price data table\n\nSPL\n\nA2: The position where the stock price is higher than those of the previous and next 5 days.\nA3: Calculate the increase at that time.\nSQL\nSELECT closing/closing_pre-1 AS raise\nFROM(\n    SELECT dt, closing, ROWNUM AS rn,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING) AS max_pre,\n        MAX(closing) OVER (\n            ORDER BY dt ROWS BETWEEN 1 FOLLOWING AND 5 FOLLOWING) AS max_suf,\n        LAG(closing,1) OVER (ORDER BY dt) AS closing_pre\n    FROM stock)\nWHERE rn>5 AND rn<=(select count(*) FROM stock)-5\n    AND CLOSING>max_pre AND CLOSING>max_suf;\n\nPython\nstock_price_df = pd.read_csv('STOCK.csv')\nprice_increase_list = []\nfor i in range(5, len(stock_price_df)-5):\n    if stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i-5:i]) and \\\n    stock_price_df['CLOSING'][i] > max(stock_price_df['CLOSING'][i+1:i+6]):\n        price_increase = stock_price_df['CLOSING'][i] / stock_price_df['CLOSING'][i-1]-1\n        price_increase_list.append(price_increase)\nresult = price_increase_list\n\nesProc SPL is open-source and here's the Open-Source Address.",
    "category": "äº§å“"
  },
  {
    "title": "Getting out of the freelancing game by building a $100k+ MRR Shopify app portfolio",
    "content": "CompanyKaching AppzFounderErikas MaliÅ¡auskasRevenue>$100K a monthErikas MaliÅ¡auskas was a successful freelancer who wanted to build something for himself. After a few failed attempts, he had a small success which got acquired. He then used that money to get a portfolio of Shopify apps off the ground.Now, the Kaching Appz portfolio is bringing in a 6-figure MRR. Here's Erikas on how he did it. ğŸ‘‡ContentsFrom freelancer to indie hackerComing up with an ideaShopify tech stackPricing rightGrowing with a focus on productParting adviceWhat's next?From freelancer to indie hackerFor most of my career, I was quite a successful freelance designer, and I had a design agency as well. Financially, I was doing well, but I really wanted to work on my own products instead of doing client work. I believed that was the way to scale my income indefinitely.So, since 2018, Iâ€™ve been trying to build successful products to replace my clients. Several of my projects failed before I launched my first success in 2021. It was a Shopify app that I scaled to $6.5k MRR and sold for $250k.I used that experience to launch another app, Kaching Bundle Quantity Breaks, which is currently at a 6-figure MRR. And weâ€™re building more Shopify apps under the Kaching Appz name.I also have other hobby projects such as my screenshot editor, PimpMySnap.Coming up with an ideaWhen I was freelancing, a lot of my clients were doing e-commerce on Shopify, so I got to know the Shopify ecosystem from the inside.When I stumbled upon the Shopify partners page, I saw an interesting banner stating that the top 25% Shopify app developers earn $272K annually. Having used quite a few Shopify apps myself, I knew I could do better with all of my UX experience.I dedicated significant time to brainstorming ideas, analyzing the competitive landscape, and researching popular app store search queries. My main criteria were:Development complexity - I wanted a product that could be built quickly so I could launch, test, and iterate rapidly.Target audience size - the solution needed to be scalable, addressing a widespread problem that every store could benefit from, rather than catering to a niche audience of just 30 people.Competition landscape - competing against apps with hundreds of 5-star reviews would be incredibly challenging, so I aimed to find a space with less established competition.Eventually, I decided I will make a simple app to add icons to a store. Whether itâ€™s product feature icons or store guarantees, I made it easy to create a block with the app and place it anywhere. There were only a couple of apps doing that and both were a bit outdated, with fewer than 30 reviews at the time.Â It seemed like a good opportunity, so I drafted the app's design and found a local tech cofounder who could turn my designs into a working product. We spent a few weeks building a working prototype and launching it in the Shopify app store.Shopify tech stackOur Shopify apps have two main parts: an iframe embedded in the Shopify admin for merchants, and a storefront widget integrated right into the shopâ€™s theme for customers. For our Bundles app admin, we started with Ruby on Rails, React, Postgres, and Heroku â€” mostly because Shopify offered great Rails support at the time, and it fit well with our skillset. These days, weâ€™re moving to Remix and TypeScript for new apps, since thatâ€™s now Shopifyâ€™s recommended approach.On the storefront side, we use Svelte. It compiles into a small web component that doesnâ€™t require a runtime, which helps keep load times low â€” and we can reuse that same component in the admin area for previews. We store all configuration details in Shopify metafields instead of making server calls, which helps maintain speed and reliability. For discounts, we rely on Shopify Functions, which run our TypeScript code millions of times each day on Shopifyâ€™s infrastructure. This setup makes it easy to scale to tens of thousands of users without driving up costs, and we never have to worry about Black Friday or other high-traffic surges.When choosing our tools, we consider what Shopify recommends, whether we can reuse our existing tech, our teamâ€™s familiarity with it, and how mature it is. Following these guidelines helps us build apps that are fast, reliable, and fit seamlessly into Shopifyâ€™s ecosystem.Pricing rightAll of our apps are subscription based and have a few tiers based on usage/value. We only earn more when our users earn more.For example, our Basic plan costs $14.99 and it comes with all of the features. Once merchants get $1,000 additional revenue through our app, theyâ€™re upgraded to our Scale plan at $29.00. Itâ€™s a tremendous 34x return on investment for them. We have merchants making literally millions of dollars with our app paying only $59.99.I think testing pricing is crucial for any business. I remember one of my friends in eCom told a story about how they increased their pricing by 30% and the conversion rate didnâ€™t change at all. But I hate raising prices â€œjust because you canâ€, so I always introduce a big new feature before a price increase. I also always grandfather our existing customers in so they never get a price increase.Weâ€™re an extremely lean company. We always keep our profitability at 90%. We still have no office, no in-house employees, no agencies weâ€™re working with.I think most people start spending more when they earn more, just because they can. Fancy offices, over-hiring, etc. Then, company effectiveness drops, and slowly, you become just another corporation. I hate that. Weâ€™re trying to hustle as hard as our first day with $0 MRR.Growing with a focus on productProbably the biggest challenge for every indie maker is getting their first users. For us, one thing was a huge success: manual outreach via Facebook groups.We just shilled our app in dropshipping groups. We gave it out for free. Some of the posts went semi-viral which resulted in our app being one of the most trending apps in the Shopify app store, and that brought us even more installs. We managed to get 1,000 users in the first month.I learned that doing things that donâ€™t scale is the key in the beginning. We reached out to every user manually, did all the customer support ourselves, and gathered a lot of feedback. That helped us shape our product before introducing monetization.After all, we're a product-led company with no marketing team. We believe that once you have an initial userbase, you just need to build a great product and make your users happy. Word of mouth will kick in. Our app has been featured by quite a few e-commerce gurus in various videos and courses without us even knowing about it.We currently get around 15k installs a month. 50% of that comes from word of mouth â€” we can attribute it to the branded search keyword â€œkachingâ€. Another 40% comes from other organic keywords, and 10% comes from affiliate partnerships and ads.Parting adviceBuild what you knowFind a niche youâ€™re comfortable with and build around that. Itâ€™s always easier to build for yourself or someone close who you know really well. Market knowledge is a game changer. I donâ€™t think I would have been successful without previous experience in Shopify.Don't over-researchDonâ€™t overthink researching things and finding a perfect fit. I donâ€™t believe in a â€œperfect ideaâ€. Instead of spending weeks researching, just give it a try, build an MVP in a few weeks and see how it goes. Even if itâ€™s a failure, you will learn so much more from the process than from the research.Focus on the customerShipping fast and focusing on customer feedback is the key. A few months ago we even introduced a feature suggestion board for every app of ours. Almost all the things that we end up building now come from there.BootstrapIf youâ€™re looking for financial freedom, avoid raising money from VCs. You will never be free with a VC over your head. A $10K MRR product could be a life changer for most founders, enabling financial freedom. But no VC firm would be happy with $10K MRR, so they wouldnâ€™t ever let you take the profits. You would be forced to spend everything on growth, raise more money, and then burn that for growth. Bootstrapping makes more sense.Find a cofounderOne of my mistakes early on was hiring a development contractor instead of having a cofounder. Soon enough, I realized that itâ€™s too hard to manage them. It was too much time and money. So, after a couple of months struggling with the contractor and not launching the app, I found a cofounder. We launched an app with him in just a few weeks and weâ€™re still working together more than four years later.Read theseI believe that every founder has to read Paul Grahamâ€™s essays and Startup Playbook by Sam Altman.What's next?My goal is to reach a $1M MRR with KachingAppz. Optimistically, I believe we could hit it before 2026. Our vision is having a portfolio of Shopify apps in the upsell/discount category. All of those apps will be well integrated and cross promoted.Besides that, I really want to personally run a startup studio where all of my ideas could come to reality. I already have a couple of side projects like PimpMySnap. Iâ€™m also building a couple of Webflow apps as we speak. Iâ€™m still curious about building stuff, therefore I will continue doing that.I just need a proper system for this because my personal time is very limited and currently all of the projects I do require a lot of involvement from me.If you want to follow along, Iâ€™m sharing my entrepreneurship journey on my X. There, you will find all my ideas, daily struggles, failures and successes.Indie Hackers Newsletter: Subscribe to get the latest stories, trends, and insights for indie hackers in your inbox 3x/week.",
    "url": "https://www.indiehackers.com/post/tech/getting-out-of-the-freelancing-game-by-building-a-100k-mrr-shopify-app-portfolio-qdReVAgLjz6EpW4OrJSI",
    "crawl_time": "2025-02-10 06:42:27",
    "translated_content": "CompanyKaching AppzFounderErikas MaliÅ¡auskasRevenue>$100K a monthErikas MaliÅ¡auskas was a successful freelancer who wanted to build something for himself. After a few failed attempts, he had a small success which got acquired. He then used that money to get a portfolio of Shopify apps off the ground.Now, the Kaching Appz portfolio is bringing in a 6-figure MRR. Here's Erikas on how he did it. ğŸ‘‡ContentsFrom freelancer to indie hackerComing up with an ideaShopify tech stackPricing rightGrowing with a focus on productParting adviceWhat's next?From freelancer to indie hackerFor most of my career, I was quite a successful freelance designer, and I had a design agency as well. Financially, I was doing well, but I really wanted to work on my own products instead of doing client work. I believed that was the way to scale my income indefinitely.So, since 2018, Iâ€™ve been trying to build successful products to replace my clients. Several of my projects failed before I launched my first success in 2021. It was a Shopify app that I scaled to $6.5k MRR and sold for $250k.I used that experience to launch another app, Kaching Bundle Quantity Breaks, which is currently at a 6-figure MRR. And weâ€™re building more Shopify apps under the Kaching Appz name.I also have other hobby projects such as my screenshot editor, PimpMySnap.Coming up with an ideaWhen I was freelancing, a lot of my clients were doing e-commerce on Shopify, so I got to know the Shopify ecosystem from the inside.When I stumbled upon the Shopify partners page, I saw an interesting banner stating that the top 25% Shopify app developers earn $272K annually. Having used quite a few Shopify apps myself, I knew I could do better with all of my UX experience.I dedicated significant time to brainstorming ideas, analyzing the competitive landscape, and researching popular app store search queries. My main criteria were:Development complexity - I wanted a product that could be built quickly so I could launch, test, and iterate rapidly.Target audience size - the solution needed to be scalable, addressing a widespread problem that every store could benefit from, rather than catering to a niche audience of just 30 people.Competition landscape - competing against apps with hundreds of 5-star reviews would be incredibly challenging, so I aimed to find a space with less established competition.Eventually, I decided I will make a simple app to add icons to a store. Whether itâ€™s product feature icons or store guarantees, I made it easy to create a block with the app and place it anywhere. There were only a couple of apps doing that and both were a bit outdated, with fewer than 30 reviews at the time.Â It seemed like a good opportunity, so I drafted the app's design and found a local tech cofounder who could turn my designs into a working product. We spent a few weeks building a working prototype and launching it in the Shopify app store.Shopify tech stackOur Shopify apps have two main parts: an iframe embedded in the Shopify admin for merchants, and a storefront widget integrated right into the shopâ€™s theme for customers. For our Bundles app admin, we started with Ruby on Rails, React, Postgres, and Heroku â€” mostly because Shopify offered great Rails support at the time, and it fit well with our skillset. These days, weâ€™re moving to Remix and TypeScript for new apps, since thatâ€™s now Shopifyâ€™s recommended approach.On the storefront side, we use Svelte. It compiles into a small web component that doesnâ€™t require a runtime, which helps keep load times low â€” and we can reuse that same component in the admin area for previews. We store all configuration details in Shopify metafields instead of making server calls, which helps maintain speed and reliability. For discounts, we rely on Shopify Functions, which run our TypeScript code millions of times each day on Shopifyâ€™s infrastructure. This setup makes it easy to scale to tens of thousands of users without driving up costs, and we never have to worry about Black Friday or other high-traffic surges.When choosing our tools, we consider what Shopify recommends, whether we can reuse our existing tech, our teamâ€™s familiarity with it, and how mature it is. Following these guidelines helps us build apps that are fast, reliable, and fit seamlessly into Shopifyâ€™s ecosystem.Pricing rightAll of our apps are subscription based and have a few tiers based on usage/value. We only earn more when our users earn more.For example, our Basic plan costs $14.99 and it comes with all of the features. Once merchants get $1,000 additional revenue through our app, theyâ€™re upgraded to our Scale plan at $29.00. Itâ€™s a tremendous 34x return on investment for them. We have merchants making literally millions of dollars with our app paying only $59.99.I think testing pricing is crucial for any business. I remember one of my friends in eCom told a story about how they increased their pricing by 30% and the conversion rate didnâ€™t change at all. But I hate raising prices â€œjust because you canâ€, so I always introduce a big new feature before a price increase. I also always grandfather our existing customers in so they never get a price increase.Weâ€™re an extremely lean company. We always keep our profitability at 90%. We still have no office, no in-house employees, no agencies weâ€™re working with.I think most people start spending more when they earn more, just because they can. Fancy offices, over-hiring, etc. Then, company effectiveness drops, and slowly, you become just another corporation. I hate that. Weâ€™re trying to hustle as hard as our first day with $0 MRR.Growing with a focus on productProbably the biggest challenge for every indie maker is getting their first users. For us, one thing was a huge success: manual outreach via Facebook groups.We just shilled our app in dropshipping groups. We gave it out for free. Some of the posts went semi-viral which resulted in our app being one of the most trending apps in the Shopify app store, and that brought us even more installs. We managed to get 1,000 users in the first month.I learned that doing things that donâ€™t scale is the key in the beginning. We reached out to every user manually, did all the customer support ourselves, and gathered a lot of feedback. That helped us shape our product before introducing monetization.After all, we're a product-led company with no marketing team. We believe that once you have an initial userbase, you just need to build a great product and make your users happy. Word of mouth will kick in. Our app has been featured by quite a few e-commerce gurus in various videos and courses without us even knowing about it.We currently get around 15k installs a month. 50% of that comes from word of mouth â€” we can attribute it to the branded search keyword â€œkachingâ€. Another 40% comes from other organic keywords, and 10% comes from affiliate partnerships and ads.Parting adviceBuild what you knowFind a niche youâ€™re comfortable with and build around that. Itâ€™s always easier to build for yourself or someone close who you know really well. Market knowledge is a game changer. I donâ€™t think I would have been successful without previous experience in Shopify.Don't over-researchDonâ€™t overthink researching things and finding a perfect fit. I donâ€™t believe in a â€œperfect ideaâ€. Instead of spending weeks researching, just give it a try, build an MVP in a few weeks and see how it goes. Even if itâ€™s a failure, you will learn so much more from the process than from the research.Focus on the customerShipping fast and focusing on customer feedback is the key. A few months ago we even introduced a feature suggestion board for every app of ours. Almost all the things that we end up building now come from there.BootstrapIf youâ€™re looking for financial freedom, avoid raising money from VCs. You will never be free with a VC over your head. A $10K MRR product could be a life changer for most founders, enabling financial freedom. But no VC firm would be happy with $10K MRR, so they wouldnâ€™t ever let you take the profits. You would be forced to spend everything on growth, raise more money, and then burn that for growth. Bootstrapping makes more sense.Find a cofounderOne of my mistakes early on was hiring a development contractor instead of having a cofounder. Soon enough, I realized that itâ€™s too hard to manage them. It was too much time and money. So, after a couple of months struggling with the contractor and not launching the app, I found a cofounder. We launched an app with him in just a few weeks and weâ€™re still working together more than four years later.Read theseI believe that every founder has to read Paul Grahamâ€™s essays and Startup Playbook by Sam Altman.What's next?My goal is to reach a $1M MRR with KachingAppz. Optimistically, I believe we could hit it before 2026. Our vision is having a portfolio of Shopify apps in the upsell/discount category. All of those apps will be well integrated and cross promoted.Besides that, I really want to personally run a startup studio where all of my ideas could come to reality. I already have a couple of side projects like PimpMySnap. Iâ€™m also building a couple of Webflow apps as we speak. Iâ€™m still curious about building stuff, therefore I will continue doing that.I just need a proper system for this because my personal time is very limited and currently all of the projects I do require a lot of involvement from me.If you want to follow along, Iâ€™m sharing my entrepreneurship journey on my X. There, you will find all my ideas, daily struggles, failures and successes.Indie Hackers Newsletter: Subscribe to get the latest stories, trends, and insights for indie hackers in your inbox 3x/week.",
    "category": "å…¶ä»–"
  },
  {
    "title": "From viral side project to a $5M/yr B2B AI platform",
    "content": "CompanyChatbaseFounderYasser ElsaidRevenue>$417K a monthTwo years ago, Yasser Elsaid launched Chatbase, and within just six months, he hit $64k MRR. Now, as he relaunches it on his two-year anniversary, he's at $5M ARR.Here's Yasser on how he did it. ğŸ‘‡ContentsAccidentally viralPivoting the MVPTwo years laterOrganic growthParting adviceWhat's next?Accidentally viralIn February 2023, I was in my final year of university, and I had a full course load. I didnâ€™t receive a return offer from my Meta internship, which, in hindsight, was the best thing that ever happened to me.I was inspired by some big names in the indie hacking space that were getting into AI, like Pieter Levels. I then started exploring how to build useful tools using OpenAIâ€™s API.The idea for Chatbase started as â€œChatGPT for your PDFs,â€ a simple tool that allowed users to upload a document and chat with its contents. I tweeted about it to my 16 followers, and to my surprise, it went viral.I realized I had hit on something with huge potential, and within weeks, I decided to make Chatbase my full-time focus, even at the cost of failing two of my final classes.It was worth it.Pivoting the MVPBuilding the MVP for Chatbase took about two months. The first version was simple but effective: Upload a PDF and the tool would create a chatbot that could answer questions based on the documentâ€™s content.I used React, Next.js, Supabase, OpenAIâ€™s API, LangChain, and Pinecone to bring it to life. It was a lean, cost-effective stack that allowed me to move quickly.The viral success of my launch tweet showed me that there was real demand. However, I quickly realized that focusing on PDFs alone wouldnâ€™t be enough. I needed to stay ahead of the competition.When Chatbase launched, it was the first â€œChat with PDFâ€ SaaS tool. Within months, competitors flooded the space, and it became clear that differentiation was critical.This realization led to a pivot toward building a platform for customer-facing AI agents, which has a much higher technical barrier to entry, a larger market, and greater long-term potential.Two years laterNow, Chatbase is a platform that allows businesses to create AI-powered customer support agents. These agents arenâ€™t just bots that answer questions â€” theyâ€™re capable of taking actions, like updating subscriptions, rescheduling appointments, or handling customer requests in real time.It is a subscription-based platform with tiered pricing, starting from $40/month plan up to an enterprise plan. The bulk of our revenue comes from higher-tier plans, where businesses use our API integrations and advanced features to create fully-customized AI agents.Revenue growth has been consistent, scaling from $64,000 MRR in May 2023 to over $5M ARR today. Much of this growth has been driven by the shift from simple chatbots to AI agents that can perform actions, which adds tremendous value for businesses.Organic growthThe initial launch of Chatbase went viral thanks to the perfect storm of being early in the AI space and riding the ChatGPT hype.Tweets and posts on platforms like Indie Hackers, Product Hunt, Reddit, and AI directories drove the first wave of users. AI influencers also shared Chatbase organically, which helped immensely.After the initial wave, we focused on long-term growth strategies:SEO and content marketing: Building a library of resources around AI customer support and chatbot implementation.Direct partnerships: Collaborating with companies for case studies and adding logos to our site for social proof.Customer feedback loops: Listening closely to users and iterating quickly to add features that meet their needs.These efforts have paid off, and today weâ€™re serving over 9,000 active subscriptions.Parting adviceLaunch early and often: The sooner you get something in front of users, the sooner you can learn what works.Listen to your customers: Theyâ€™ll tell you what they need if youâ€™re paying attention.Focus on high-barrier opportunities: Spaces with low competition but high potential are where you can really thrive.Build in public: Sharing your journey not only helps you grow an audience but also connects you with like-minded people who can support you.What's next?As we relaunch Chatbase on its two-year anniversary, February 4, 2025, our goal is to cement our position as the leading platform for building customer-facing AI agents. We plan to:Expand integrations with tools businesses already use, like Stripe and Cal.com.Have better functionality around the AI model for specific use cases like SaaS support.You can follow along on X and LinkedIn. And check out Chatbase!Indie Hackers Newsletter: Subscribe to get the latest stories, trends, and insights for indie hackers in your inbox 3x/week.",
    "url": "https://www.indiehackers.com/post/tech/from-viral-side-project-to-a-5m-yr-b2b-ai-platform-TpbhTVyp1sjBk0uzo4tR",
    "crawl_time": "2025-02-10 06:42:31",
    "translated_content": "CompanyChatbaseFounderyAsser ElsaidRevenue>æ¯æœˆ41.7ä¸‡ç¾å…ƒï¼ŒYasser Elsaidæ¨å‡ºäº†Chatbaseï¼Œåœ¨çŸ­çŸ­å…­ä¸ªæœˆå†…ï¼Œä»–è¾¾åˆ°äº†6.4K MRRã€‚ç°åœ¨ï¼Œéšç€ä»–åœ¨ä»–ä¸¤å‘¨å¹´çºªå¿µæ—¥é‡æ–°å¯åŠ¨æ—¶ï¼Œä»–çš„Yasserä¸º500ä¸‡ç¾å…ƒã€‚ contentsAccccccccccccccccced proverpivotivoting Mvptwoå¹´çš„åŸ¹è®­ç–—æ³•åŸ¹è®­ä¸“ä¸šå»ºè®®ä¸‹ä¸€ä¸ªæ˜¯ä»€ä¹ˆï¼Ÿæˆ‘æ²¡æœ‰æ”¶åˆ°æˆ‘çš„Metaå®ä¹ çš„å›æŠ¥æŠ¥ä»·ï¼Œäº‹åçœ‹æ¥ï¼Œè¿™æ˜¯æˆ‘å‘ç”Ÿè¿‡çš„æœ€å¥½çš„äº‹æƒ…ã€‚æˆ‘çš„çµæ„Ÿæ¥è‡ªç‹¬ç«‹é»‘å®¢å…¥ä¾µç©ºé—´ä¸­çš„ä¸€äº›çŸ¥åäººå£«ï¼Œè¿™äº›ç©ºé—´æ­£åœ¨è¿›å…¥AIï¼Œä¾‹å¦‚Pieterçº§åˆ«ã€‚ç„¶åï¼Œæˆ‘å¼€å§‹æ¢ç´¢å¦‚ä½•ä½¿ç”¨OpenAIçš„APIæ„å»ºæœ‰ç”¨çš„å·¥å…·ã€‚chatbaseçš„æƒ³æ³•æœ€åˆæ˜¯â€œç”¨äºæ‚¨çš„PDFSçš„ChatGptâ€ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„å·¥å…·ï¼Œå…è®¸ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£å¹¶ä¸å…¶å†…å®¹èŠå¤©ã€‚æˆ‘å‘æˆ‘çš„16ä½è¿½éšè€…å‘äº†æ¨æ–‡ï¼Œä»¤æˆ‘æƒŠè®¶çš„æ˜¯ï¼Œæˆ‘æ„è¯†åˆ°æˆ‘å·²ç»å‡»ä¸­äº†ä¸€äº›æ½œåŠ›å·¨å¤§çš„äº‹æƒ…ï¼Œå‡ å‘¨ä¹‹å†…ï¼Œæˆ‘å†³å®šå°†chatbaseæˆä¸ºæˆ‘çš„å…¨èŒé‡ç‚¹ï¼Œå³ä½¿æ˜¯ä¸ºäº†å¤±è´¥çš„ä»£ä»·æˆ‘çš„æœ€åä¸¤ä¸ªè¯¾ç¨‹å€¼å¾—ã€‚è¿™æ˜¯å€¼å¾—çš„ã€‚ç¬¬ä¸€ä¸ªç‰ˆæœ¬å¾ˆç®€å•ä½†æœ‰æ•ˆï¼šä¸Šä¼ PDFï¼Œè¯¥å·¥å…·å°†åˆ›å»ºä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œå¯ä»¥æ ¹æ®æ–‡æ¡£çš„å†…å®¹å›ç­”é—®é¢˜ã€‚æˆ‘ä½¿ç”¨äº†Reactï¼ŒNext.jsï¼ŒSupabaseï¼ŒOpenaiçš„APIï¼ŒLangchainå’ŒPineconeï¼Œä»¥å°†å…¶æ ©æ ©å¦‚ç”Ÿã€‚è¿™æ˜¯ä¸€ä¸ªç²¾ç®€ï¼Œå…·æœ‰æˆæœ¬æ•ˆç›Šçš„å †æ ˆï¼Œä½¿æˆ‘èƒ½å¤Ÿå¿«é€Ÿç§»åŠ¨ã€‚æˆ‘çš„å‘å¸ƒæ¨æ–‡çš„ç—…æ¯’æˆåŠŸè¡¨æ˜æˆ‘æœ‰çœŸæ­£çš„éœ€æ±‚ã€‚ä½†æ˜¯ï¼Œæˆ‘å¾ˆå¿«æ„è¯†åˆ°ï¼Œä»…ä¸“æ³¨äºPDFæ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘éœ€è¦åœ¨æ¯”èµ›ä¸­ä¿æŒé¢†å…ˆã€‚å‡ ä¸ªæœˆä¹‹å†…ï¼Œç«äº‰å¯¹æ‰‹æ·¹æ²¡äº†è¿™ä¸ªç©ºé—´ï¼Œå¾ˆæ˜æ˜¾å·®å¼‚åŒ–è‡³å…³é‡è¦ã€‚è¿™å¯¼è‡´äº†å»ºç«‹é¢å‘å®¢æˆ·AIä»£ç†å¹³å°çš„æ¢çº½ï¼Œè¯¥å¹³å°å…·æœ‰æ›´é«˜çš„æŠ€æœ¯éšœç¢ï¼Œæ›´å¤§çš„å¸‚åœºå’Œæ›´å¤§çš„å¸‚åœºéšœç¢é•¿æœŸçš„æ½œåŠ›ã€‚ä¸¤å¹´åï¼ŒChatbaseæ˜¯ä¸€ä¸ªå¹³å°ï¼Œå…è®¸ä¼ä¸šåˆ›å»ºAIé©±åŠ¨çš„å®¢æˆ·æ”¯æŒä»£ç†å•†ã€‚è¿™äº›ä»£ç†ä¸ä»…å¯ä»¥å›ç­”é—®é¢˜çš„æœºå™¨äºº - å®ƒä»¬èƒ½å¤Ÿé‡‡å–è¡ŒåŠ¨ï¼Œä¾‹å¦‚æ›´æ–°è®¢é˜…ï¼Œé‡æ–°å®‰æ’çº¦ä¼šæˆ–å®æ—¶å¤„ç†å®¢æˆ·è¯·æ±‚ã€‚è®¡åˆ’ä¼ä¸šè®¡åˆ’ã€‚æˆ‘ä»¬çš„å¤§éƒ¨åˆ†æ”¶å…¥æ¥è‡ªé«˜å±‚è®¡åˆ’ï¼Œåœ¨è¯¥è®¡åˆ’ä¸­ï¼Œä¼ä¸šä½¿ç”¨æˆ‘ä»¬çš„APIé›†æˆå’Œé«˜çº§åŠŸèƒ½æ¥åˆ›å»ºå®Œå…¨å®šåˆ¶çš„AIä»£ç†ã€‚æ”¶ç›Šçš„å¢é•¿ä¸€ç›´ä¿æŒä¸€è‡´ï¼Œä»2023å¹´5æœˆçš„64,000ç¾å…ƒæ‰©å±•åˆ°ä»Šå¤©çš„500ä¸‡ç¾å…ƒARRã€‚è¿™ç§å¢é•¿çš„å¤§éƒ¨åˆ†æ˜¯ç”±ä»ç®€å•çš„èŠå¤©æœºå™¨äººè½¬ç§»åˆ°å¯ä»¥æ‰§è¡ŒåŠ¨ä½œçš„AIä»£ç†çš„è½¬å˜æ‰€é©±åŠ¨çš„ï¼Œè¿™ä¸ºä¸šåŠ¡å¢æ·»äº†å·¨å¤§çš„ä»·å€¼ã€‚æœ‰æœºå¢é•¿ï¼ŒChatbaseçš„æœ€åˆå‘å°„é€æ¸æµè¡Œï¼Œè¿™è¦å½’åŠŸäºåœ¨AIç©ºé—´æ—©æœŸå’Œéª‘é©¬çš„æ—©æœŸé£æš´Chatgpt Hype.tweetså’Œè¯¸å¦‚Indie Hackersï¼ŒProduct Huntï¼ŒRedditå’ŒAIç›®å½•ç­‰å¹³å°ä¸Šçš„å¸–å­é©±åŠ¨äº†ç¬¬ä¸€æ³¢ç”¨æˆ·ã€‚äººå·¥æ™ºèƒ½å½±å“è€…è¿˜æœ‰æœºåœ°å…±äº«äº†chatbaseï¼Œè¿™å¯¹æœ€åˆçš„æµªæ½®äº§ç”Ÿäº†æå¤§çš„å¸®åŠ©ã€‚æˆ‘ä»¬ä¸“æ³¨äºé•¿æœŸå¢é•¿ç­–ç•¥ï¼šSEOå’Œå†…å®¹è¥é”€ï¼šå›´ç»•AIå®¢æˆ·æ”¯æŒå’ŒèŠå¤©æœºå™¨äººå®æ–½å»ºç«‹èµ„æºåº“ã€‚ç›´æ¥åˆä½œä¼™ä¼´å…³ç³»ï¼šä¸å…¬å¸ä¸å…¬å¸åˆä½œï¼Œä»¥äº‰å–ä¸ºå…¬å¸åˆä½œè€Œè¿›è¡Œã€‚æ¡ˆä¾‹ç ”ç©¶å¹¶å°†å¾½æ ‡æ·»åŠ åˆ°æˆ‘ä»¬çš„ç½‘ç«™ä»¥è·å¾—ç¤¾äº¤è¯æ˜ã€‚customeråé¦ˆå¾ªç¯ï¼šä»”ç»†è†å¬ç”¨æˆ·å¹¶è¿…é€Ÿè¿­ä»£ä»¥æ·»åŠ æ»¡è¶³ä»–ä»¬éœ€æ±‚çš„åŠŸèƒ½ã€‚è¿™äº›åŠªåŠ›å¾—åˆ°äº†å›æŠ¥ï¼Œä»Šå¤©æˆ‘ä»¬æ­£åœ¨ä¸º9,000å¤šä¸ªæ´»åŠ¨æä¾›æœåŠ¡ã€‚æ—©æœŸå’Œç»å¸¸ï¼šæ‚¨è¶Šæ—©åœ¨ç”¨æˆ·é¢å‰å¾—åˆ°ä¸€äº›ä¸œè¥¿ï¼Œæ‚¨å°±å¯ä»¥è¶Šæ—©äº†è§£æœ‰æ•ˆçš„ä½œç”¨ã€‚å‘å®¢æˆ·è¯´ï¼šä»–ä»¬ä¼šå‘Šè¯‰æ‚¨ä»–ä»¬éœ€è¦æ³¨æ„çš„æ˜¯ä»€ä¹ˆã€‚åœ¨ç«äº‰è¾ƒä½ä½†æ½œåŠ›è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥çœŸæ­£è“¬å‹ƒå‘å±•ã€‚åœ¨å…¬å…±åœºåˆå»ºç«‹ï¼šåˆ†äº«æ‚¨çš„æ—…ç¨‹ä¸ä»…å¯ä»¥å¸®åŠ©æ‚¨å‘å±•è§‚ä¼—ï¼Œè¿˜å¯ä»¥å°†æ‚¨ä¸å¿—è¶£ç›¸æŠ•çš„äººè”ç³»åœ¨ä¸€èµ·ã€‚æ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆï¼Ÿæ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆï¼Ÿ 2025å¹´2æœˆ4æ—¥ï¼Œä¸¤å‘¨å¹´çºªå¿µæ—¥ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å·©å›ºæˆ‘ä»¬ä½œä¸ºå»ºç«‹é¢å‘å®¢æˆ·AIä»£ç†å•†çš„é¢†å…ˆå¹³å°çš„åœ°ä½ã€‚æˆ‘ä»¬è®¡åˆ’ï¼šæ‰©å±•ä¸å·²ç»ä½¿ç”¨çš„å·¥å…·ç›¸é›†æˆçš„é›†æˆï¼Œä¾‹å¦‚Stripeå’ŒCal.comã€‚åœ¨AIæ¨¡å‹å‘¨å›´å…·æœ‰æ›´å¥½çš„åŠŸèƒ½ï¼Œä¾‹å¦‚SaaS Supportç­‰ç‰¹å®šç”¨ä¾‹ã€‚æ‚¨å¯ä»¥åœ¨Xå’ŒLinkedInä¸Šéµå¾ªã€‚å¹¶æŸ¥çœ‹chatbaseï¼ç‹¬ç«‹é»‘å®¢é€šè®¯ï¼šè®¢é˜…ä»¥è·å–æ”¶ä»¶ç®±3x/å‘¨çš„ç‹¬ç«‹é»‘å®¢çš„æœ€æ–°æ•…äº‹ï¼Œè¶‹åŠ¿å’Œè§è§£ã€‚",
    "category": "æŠ€æœ¯"
  },
  {
    "title": "Freeing Data Scientists from Heavy Coding: A New Era of Automation",
    "content": "SQL is wasting lives of data scientists\nSQL is difficult to write\nNearly all data scientists use SQL for data exploration and analysis. SQL appears deceptively simple and offers a certain degree of interactivity, making it a seemingly ideal choice for these purposes.\nTo perform filtering and grouping operations, for example, SQL just needs one line of code:\nselect id,name from T where id=1\nselect area,sum(amount) from T group by area\n\nBut this is only limited to simple cases. When the computation becomes complex, SQL code becomes complicated, too. For example, to count the longest consecutive rising days of each stock, the SQL statement is as follows:\nSELECT CODE, MAX(con_rise) AS longest_up_days\nFROM (\n    SELECT CODE, COUNT(*) AS con_rise\n    FROM (\n        SELECT CODE, DT, SUM(updown_flag) OVER (PARTITION BY CODE ORDER BY CODE, DT) AS no_up_days\n        FROM (\n            SELECT CODE, DT, \n                CASE WHEN CL > LAG(CL) OVER (PARTITION BY CODE ORDER BY CODE, DT)  THEN 0\n                ELSE 1 END AS updown_flag\n            FROM stock\n        )\n    )\n    GROUP BY CODE, no_up_days\n)\nGROUP BY CODE\n\nAnd to perform the commonly seen funnel analysis on user behavior data in e-commerce industries:\nWITH e1 AS (\n\tSELECT uid,1 AS step1, MIN(etime) AS t1\n\tFROM events\n\tWHERE etime>=end_date-14 AND etime<end_date AND etype='etype1'\n\tGROUP BY uid),\ne2 AS (\n\tSELECT uid,1 AS step2, MIN(e1.t1) as t1, MIN(e2.etime) AS t2\n\tFROM events AS e2 JOIN e1 ON e2.uid = e1.uid\n\tWHERE e2.etime>=end_date-14 AND e2.etime<end_date AND e2.etime>t1 AND e2.etime<t1+7 AND etype='etype2'\n\tGROUP BY uid),\ne3 as (\n\tSELECT uid,1 AS step3, MIN(e2.t1) as t1, MIN(e3.etime) AS t3\n\tFROM events AS e3 JOIN e2 ON e3.uid = e2.uid\n\tWHERE e3.etime>=end_date-14 AND e3.etime<end_date AND e3.etime>t2 AND e3.etime<t1+7 AND etype='etype3'\n\tGROUP BY uid)\nSELECT SUM(step1) AS step1, SUM(step2) AS step2, SUM(step3) AS step3\nFROM e1 LEFT JOIN e2 ON e1.uid = e2.uid LEFT JOIN e3 ON e2.uid = e3.uid\n\nBoth examples involve multilayer nested subqueries, which are difficult to understand and more difficult to write.\nThere are many similar computing tasks in real-world business scenarios. For example:\n\n\nFind players who score three times continuously in one minute;\n\n\nFind the number of users in 7 days who are active in three continuous days;\n\n\nCompute retention rate of every dayâ€™s new users a business retains in the next day;\n\n\nCompute the growth rate of a stock on the date when the price is both greater than both the previous and the next five days;\n\n\nâ€¦\n\n\nThese complex requirements often necessitate multi-step procedures and involve order-based operations. Their SQL implementations are extremely roundabout with a nearly one hundred lines of N-layered nested statement. Data scientists are wasting their lives in writing such SQL statements.\nSQL is difficult to debug\nComplicated SQL statements are very inconvenient to debug, such as the above commonly seen complex nested SQL statements. To debug them, we need to disassemble the statement and perform debugging layer by layer. Thedisassembly process involves modification of the SQL statement, making the whole debugging procedure very complicated.\n\nThis is because SQL does not have common debugging methods such as â€œset breakpointâ€ and â€œstep overâ€. And data scientists have to go to the trouble of disassembling statements in order to perform debugging, which is a waste of their lives.\nSQL has low performance\nThe query performance of SQL heavily depends on database optimizer. A well-designed database product is able to automatically adopt a more efficient algorithm (instead of executing the SQL statement literally), but often the optimization mechanism breaks down when facing complex computing logics.\nHere is a simple example. To fetch the top 10 from 100 million records, SQL has the following code:\nSELECT TOP 10 x FROM T ORDER BY x DESC\n\nThough this SQL statement contains the ORDER BY keyword, database optimizer will use a more efficient algorithm instead of performing the full sorting (because big data sorting is very slow).\nNow letâ€™s modify the task to find the top 10 from each group, and SQL has the following implementation:\nSELECT * FROM (\n SELECT *, ROW_NUMBER() OVER (PARTITION BY Area ORDER BY Amount DESC) rn\n FROM Orders )\nWHERE rn<=10\n\nThe implementation only gets a little complicated, but most database optimizers already become incapable of performing the optimization. They cannot guess the SQL statementâ€™s purpose but can only execute the literal logic written in the statement to perform the sorting (as there is ORDER BY keyword). As a result, performance is sharply decreased.\nThe SQL statements in real-world business scenarios are far more complicated than that in the example. It is rather common that database optimizer becomes ineffective, such as the above SQL statement handling funnel analysis. The statement involves repeated joins, which are difficult to write and extremely slow to execute.\nLow performance means waiting. For certain big data processing cases, waiting times are from a number of hours to even one day. And during the long waits data scientistsâ€™ lives passed.\nSQL is closed\nSQL is the formal language used by databases. The closed databases make data processing difficult. Being closed here refers to the database requirement that data to be computed and processed by the database should be loaded into it in advance. The border between internal data and external data is clear.\nIn real-life businesses, data analysts often need to process data coming from the other sources, including text, Excel, application interface, web crawler, to name a few. Sometimes data coming from any of those sources is only temporarily used, but loading them into the database for each use consumes database space resources and the ETL process is time-consuming. And databases usually have constraints, and non-standardized data cannot be loaded into. This requires to first re-organize data, which needs both time and resources, and then write them to the database, which is also time-consuming (as database writes are very slow). It is during handling these peripheral data organization, loading and retrieval operations that the life of a data scientist is wasted.\nPython is also wasting lives of data scientists\nAs SQL has too many problems, data scientists seek its replacements, and Python is one of them.\nPython surpasses SQL in many aspects. It is easier to debug and more open, and supports procedural computation. However, Python also has its flaws.\nComplex computations are still hard to handle\nPandas, one of Pythonâ€™s third-party libraries, offers a rich collection of computing functions, which make certain computations simpler than their SQL counterparts. However, handling complex scenarios may still be challenging. For example, here is the code for finding the longest consecutive rising days for each stock mentioned earlier:\nimport pandas as pd\nstock_file = \"StockRecords.txt\"\nstock_info = pd.read_csv(stock_file,sep=\"\\t\")\nstock_info.sort_values(by=['CODE','DT'],inplace=True)\nstock_group = stock_info.groupby(by='CODE')\nstock_info['label'] = stock_info.groupby('CODE')['CL'].diff().fillna(0).le(0).astype(int).cumsum()\nmax_increase_days = {}\nfor code, group in stock_info.groupby('CODE'):\n\tmax_increase_days[code] = group.groupby('label').size().max() â€“ 1\nmax_rise_df = pd.DataFrame(list(max_increase_days.items()), columns=['CODE', 'max_increase_days'])\n\nThe Python code, which involves hard-coding with â€œforâ€ loop, is still cumbersome. This continues to waste lives of data scientists.\nInconvenient debugging functionalities\nPython has many IDEs and provides much-better-than-SQL debugging capabilities such as â€œbreakpointâ€, which makes it unnecessary to disassemble the code.\nBut viewing intermediate values still mainly relies on the print method, which needs to be removed after debugging. This proves to be somewhat cumbersome.\n\nIt takes longer to debug when the corresponding functionality is inconvenient to use, which results in a waste of time for data scientists.\nLow big data processing capability and performance\nPython almost does not have any big data processing capabilities. While the Pandas library can perform in-memory computations like sorting and filtering directly, it struggles with datasets larger than the available memory. In that case data needs to be segmented and processed segment by segment with hardcoding, and the code becomes very complicated.\nPython's parallelism is superficial. To harness multiple CPUs, complex multi-process parallelism is often required. But this is beyond the reach of most data scientists. Bing unable to code parallel processing, data scientists can only perform the slow serial computations and witness their time wasted.\nBoth SQL and Python are not satisfactory enough, then what can truly rescue data scientists?\nesProc SPL â€“ the rescuer of data scientists\nesProc SPL! A very tool specifically designed for structured data processing.\nSPL is characterized by conciseness, ease of understanding and convenient debugging. It supports large-scale data processing and delivers high performance, fundamentally addressing limitations of SQL and Python.\nSimple to write\nSPL offers rich data types and computing class libraries and supports procedural computation, greatly simplifying the code of implementing complex computations. To find the longest consecutive rising days for each stock, for example, SPL has the following code:\n\nThe SPL code is much shorter and does not involve the loop statement. Both data read and write are not difficult.\nTo perform the ecommerce funnel analysis:\n\nCompared with its SQL counterpart, the SPL code is more concise and versatile, and more conforms to the natural way of thinking. It can be used to handle the funnel analysis involving any number of steps. E-commerce funnel analysis with SPL is simpler and aligns more with natural thinking. This code can handle any-step funnel, offering greater simplicity and versatility compared to SQL.\nConvenient to debug\nSPL also offers comprehensive debugging capabilities, including â€œSet breakpointâ€, â€œRun to cursorâ€, â€œStep overâ€, etc. The result of each step is displayed in real-time on the right side of the IDE. This is convenient as users do not need to split away each subquery or perform manual print any more.\n\nSupport for big data processing\nSPL supports big data processing, no matter whether the data can fit into the memory or not.\nIn-memory computation:\n\nExternal memory computation:\n\nWe can see that the SPL code for the external memory computation is the same as that for the in-memory computation, requiring no additional workload.\nIt is easy and convenient to implement parallel processing in SPL. You just need to add an @m option to the serial computation code.\n\nHigh performance\nIt is easy to write code with low amount of computational work and achieves faster execution in SPL. Take the previously mentioned topN problem as an example, SPL treats topN as a kind of aggregation, eliminating full sorting from the computing logic and achieving much faster speed. This SPL capability is inherent and does not rely on the assistance of optimizer.\nGet TopN from the whole set:\nOrders.groups(;top(10;-Amount))\n\nGet TopN from each group:\n Orders.groups(Area;top(10;-Amount))\n\nThe code of getting topN from each group and that from the entire set is basically the same. Both are straightforward and runs fast.\nSPL offers a wide range of high-performance algorithms, including:\n\n\nFor search: Binary search, sequence-number-based location, index-based search, batch search, etc.\n\n\nFor traversal: Cursor filtering, multipurpose traversal, multicursor, new understanding of aggregation, order-based grouping, application cursor, column-wise computing, etc.\n\n\nFor association: Foreign-key-based pre-association, foreign key numberization, alignment sequence, big dimension table search, one-sided heaping, order-based merge, association-based location, etc.\n\n\nFor cluster computing: cluster composite table, duplicate dimension table, dimension table segmentation, load balancing, etc.\n\n\nEquipped with these algorithms, SPL can achieve a dramatic skyrocketing in computational performance. Data scientists do not need to waste their lives in long waiting.\nOpen system\nSPL is naturally open. It can directly compute any data sources as long as it can access them, including data files such as CSV and Excel, various relational and non-relational databases, and multi-layer data such as JSON and XML, and thus can perform mixed computations.\n\nWith the open system, data scientists can process data coming from various sources directly and efficiently, getting rid of the time spent in data organization and data import/export and thus increasing data processing efficiency.\nHigh portability and enterprise-friendly\nSPL also offers the proprietary file format that is high-performance and portable.\n\nIn contrast, Python lacks a proprietary storage solution. Text files are slow, and databases result in the loss of portability.\nSPL is enterprise-friendly because it is developed in pure Java. After data exploration and analysis, data scientists can integrate SPL with the application by embedding its jars into the latter, facilitating smooth transition from outside to inside.\nWith the convenient to use, easy to debug, high-performance and integration-friendly SPL, data scientists can be freed from the heavy coding work and devote more time to their own businesses.\nesProc SPL is an open-source tool, and its source code is available here.",
    "url": "https://www.indiehackers.com/post/freeing-data-scientists-from-heavy-coding-a-new-era-of-automation-2c2bbe9280",
    "crawl_time": "2025-02-10 06:42:35",
    "translated_content": "SQL is wasting lives of data scientists\nSQL is difficult to write\nNearly all data scientists use SQL for data exploration and analysis. SQL appears deceptively simple and offers a certain degree of interactivity, making it a seemingly ideal choice for these purposes.\nTo perform filtering and grouping operations, for example, SQL just needs one line of code:\nselect id,name from T where id=1\nselect area,sum(amount) from T group by area\n\nBut this is only limited to simple cases. When the computation becomes complex, SQL code becomes complicated, too. For example, to count the longest consecutive rising days of each stock, the SQL statement is as follows:\nSELECT CODE, MAX(con_rise) AS longest_up_days\nFROM (\n    SELECT CODE, COUNT(*) AS con_rise\n    FROM (\n        SELECT CODE, DT, SUM(updown_flag) OVER (PARTITION BY CODE ORDER BY CODE, DT) AS no_up_days\n        FROM (\n            SELECT CODE, DT, \n                CASE WHEN CL > LAG(CL) OVER (PARTITION BY CODE ORDER BY CODE, DT)  THEN 0\n                ELSE 1 END AS updown_flag\n            FROM stock\n        )\n    )\n    GROUP BY CODE, no_up_days\n)\nGROUP BY CODE\n\nAnd to perform the commonly seen funnel analysis on user behavior data in e-commerce industries:\nWITH e1 AS (\n\tSELECT uid,1 AS step1, MIN(etime) AS t1\n\tFROM events\n\tWHERE etime>=end_date-14 AND etime<end_date AND etype='etype1'\n\tGROUP BY uid),\ne2 AS (\n\tSELECT uid,1 AS step2, MIN(e1.t1) as t1, MIN(e2.etime) AS t2\n\tFROM events AS e2 JOIN e1 ON e2.uid = e1.uid\n\tWHERE e2.etime>=end_date-14 AND e2.etime<end_date AND e2.etime>t1 AND e2.etime<t1+7 AND etype='etype2'\n\tGROUP BY uid),\ne3 as (\n\tSELECT uid,1 AS step3, MIN(e2.t1) as t1, MIN(e3.etime) AS t3\n\tFROM events AS e3 JOIN e2 ON e3.uid = e2.uid\n\tWHERE e3.etime>=end_date-14 AND e3.etime<end_date AND e3.etime>t2 AND e3.etime<t1+7 AND etype='etype3'\n\tGROUP BY uid)\nSELECT SUM(step1) AS step1, SUM(step2) AS step2, SUM(step3) AS step3\nFROM e1 LEFT JOIN e2 ON e1.uid = e2.uid LEFT JOIN e3 ON e2.uid = e3.uid\n\nBoth examples involve multilayer nested subqueries, which are difficult to understand and more difficult to write.\nThere are many similar computing tasks in real-world business scenarios. For example:\n\n\nFind players who score three times continuously in one minute;\n\n\nFind the number of users in 7 days who are active in three continuous days;\n\n\nCompute retention rate of every dayâ€™s new users a business retains in the next day;\n\n\nCompute the growth rate of a stock on the date when the price is both greater than both the previous and the next five days;\n\n\nâ€¦\n\n\nThese complex requirements often necessitate multi-step procedures and involve order-based operations. Their SQL implementations are extremely roundabout with a nearly one hundred lines of N-layered nested statement. Data scientists are wasting their lives in writing such SQL statements.\nSQL is difficult to debug\nComplicated SQL statements are very inconvenient to debug, such as the above commonly seen complex nested SQL statements. To debug them, we need to disassemble the statement and perform debugging layer by layer. Thedisassembly process involves modification of the SQL statement, making the whole debugging procedure very complicated.\n\nThis is because SQL does not have common debugging methods such as â€œset breakpointâ€ and â€œstep overâ€. And data scientists have to go to the trouble of disassembling statements in order to perform debugging, which is a waste of their lives.\nSQL has low performance\nThe query performance of SQL heavily depends on database optimizer. A well-designed database product is able to automatically adopt a more efficient algorithm (instead of executing the SQL statement literally), but often the optimization mechanism breaks down when facing complex computing logics.\nHere is a simple example. To fetch the top 10 from 100 million records, SQL has the following code:\nSELECT TOP 10 x FROM T ORDER BY x DESC\n\nThough this SQL statement contains the ORDER BY keyword, database optimizer will use a more efficient algorithm instead of performing the full sorting (because big data sorting is very slow).\nNow letâ€™s modify the task to find the top 10 from each group, and SQL has the following implementation:\nSELECT * FROM (\n SELECT *, ROW_NUMBER() OVER (PARTITION BY Area ORDER BY Amount DESC) rn\n FROM Orders )\nWHERE rn<=10\n\nThe implementation only gets a little complicated, but most database optimizers already become incapable of performing the optimization. They cannot guess the SQL statementâ€™s purpose but can only execute the literal logic written in the statement to perform the sorting (as there is ORDER BY keyword). As a result, performance is sharply decreased.\nThe SQL statements in real-world business scenarios are far more complicated than that in the example. It is rather common that database optimizer becomes ineffective, such as the above SQL statement handling funnel analysis. The statement involves repeated joins, which are difficult to write and extremely slow to execute.\nLow performance means waiting. For certain big data processing cases, waiting times are from a number of hours to even one day. And during the long waits data scientistsâ€™ lives passed.\nSQL is closed\nSQL is the formal language used by databases. The closed databases make data processing difficult. Being closed here refers to the database requirement that data to be computed and processed by the database should be loaded into it in advance. The border between internal data and external data is clear.\nIn real-life businesses, data analysts often need to process data coming from the other sources, including text, Excel, application interface, web crawler, to name a few. Sometimes data coming from any of those sources is only temporarily used, but loading them into the database for each use consumes database space resources and the ETL process is time-consuming. And databases usually have constraints, and non-standardized data cannot be loaded into. This requires to first re-organize data, which needs both time and resources, and then write them to the database, which is also time-consuming (as database writes are very slow). It is during handling these peripheral data organization, loading and retrieval operations that the life of a data scientist is wasted.\nPython is also wasting lives of data scientists\nAs SQL has too many problems, data scientists seek its replacements, and Python is one of them.\nPython surpasses SQL in many aspects. It is easier to debug and more open, and supports procedural computation. However, Python also has its flaws.\nComplex computations are still hard to handle\nPandas, one of Pythonâ€™s third-party libraries, offers a rich collection of computing functions, which make certain computations simpler than their SQL counterparts. However, handling complex scenarios may still be challenging. For example, here is the code for finding the longest consecutive rising days for each stock mentioned earlier:\nimport pandas as pd\nstock_file = \"StockRecords.txt\"\nstock_info = pd.read_csv(stock_file,sep=\"\\t\")\nstock_info.sort_values(by=['CODE','DT'],inplace=True)\nstock_group = stock_info.groupby(by='CODE')\nstock_info['label'] = stock_info.groupby('CODE')['CL'].diff().fillna(0).le(0).astype(int).cumsum()\nmax_increase_days = {}\nfor code, group in stock_info.groupby('CODE'):\n\tmax_increase_days[code] = group.groupby('label').size().max() â€“ 1\nmax_rise_df = pd.DataFrame(list(max_increase_days.items()), columns=['CODE', 'max_increase_days'])\n\nThe Python code, which involves hard-coding with â€œforâ€ loop, is still cumbersome. This continues to waste lives of data scientists.\nInconvenient debugging functionalities\nPython has many IDEs and provides much-better-than-SQL debugging capabilities such as â€œbreakpointâ€, which makes it unnecessary to disassemble the code.\nBut viewing intermediate values still mainly relies on the print method, which needs to be removed after debugging. This proves to be somewhat cumbersome.\n\nIt takes longer to debug when the corresponding functionality is inconvenient to use, which results in a waste of time for data scientists.\nLow big data processing capability and performance\nPython almost does not have any big data processing capabilities. While the Pandas library can perform in-memory computations like sorting and filtering directly, it struggles with datasets larger than the available memory. In that case data needs to be segmented and processed segment by segment with hardcoding, and the code becomes very complicated.\nPython's parallelism is superficial. To harness multiple CPUs, complex multi-process parallelism is often required. But this is beyond the reach of most data scientists. Bing unable to code parallel processing, data scientists can only perform the slow serial computations and witness their time wasted.\nBoth SQL and Python are not satisfactory enough, then what can truly rescue data scientists?\nesProc SPL â€“ the rescuer of data scientists\nesProc SPL! A very tool specifically designed for structured data processing.\nSPL is characterized by conciseness, ease of understanding and convenient debugging. It supports large-scale data processing and delivers high performance, fundamentally addressing limitations of SQL and Python.\nSimple to write\nSPL offers rich data types and computing class libraries and supports procedural computation, greatly simplifying the code of implementing complex computations. To find the longest consecutive rising days for each stock, for example, SPL has the following code:\n\nThe SPL code is much shorter and does not involve the loop statement. Both data read and write are not difficult.\nTo perform the ecommerce funnel analysis:\n\nCompared with its SQL counterpart, the SPL code is more concise and versatile, and more conforms to the natural way of thinking. It can be used to handle the funnel analysis involving any number of steps. E-commerce funnel analysis with SPL is simpler and aligns more with natural thinking. This code can handle any-step funnel, offering greater simplicity and versatility compared to SQL.\nConvenient to debug\nSPL also offers comprehensive debugging capabilities, including â€œSet breakpointâ€, â€œRun to cursorâ€, â€œStep overâ€, etc. The result of each step is displayed in real-time on the right side of the IDE. This is convenient as users do not need to split away each subquery or perform manual print any more.\n\nSupport for big data processing\nSPL supports big data processing, no matter whether the data can fit into the memory or not.\nIn-memory computation:\n\nExternal memory computation:\n\nWe can see that the SPL code for the external memory computation is the same as that for the in-memory computation, requiring no additional workload.\nIt is easy and convenient to implement parallel processing in SPL. You just need to add an @m option to the serial computation code.\n\nHigh performance\nIt is easy to write code with low amount of computational work and achieves faster execution in SPL. Take the previously mentioned topN problem as an example, SPL treats topN as a kind of aggregation, eliminating full sorting from the computing logic and achieving much faster speed. This SPL capability is inherent and does not rely on the assistance of optimizer.\nGet TopN from the whole set:\nOrders.groups(;top(10;-Amount))\n\nGet TopN from each group:\n Orders.groups(Area;top(10;-Amount))\n\nThe code of getting topN from each group and that from the entire set is basically the same. Both are straightforward and runs fast.\nSPL offers a wide range of high-performance algorithms, including:\n\n\nFor search: Binary search, sequence-number-based location, index-based search, batch search, etc.\n\n\nFor traversal: Cursor filtering, multipurpose traversal, multicursor, new understanding of aggregation, order-based grouping, application cursor, column-wise computing, etc.\n\n\nFor association: Foreign-key-based pre-association, foreign key numberization, alignment sequence, big dimension table search, one-sided heaping, order-based merge, association-based location, etc.\n\n\nFor cluster computing: cluster composite table, duplicate dimension table, dimension table segmentation, load balancing, etc.\n\n\nEquipped with these algorithms, SPL can achieve a dramatic skyrocketing in computational performance. Data scientists do not need to waste their lives in long waiting.\nOpen system\nSPL is naturally open. It can directly compute any data sources as long as it can access them, including data files such as CSV and Excel, various relational and non-relational databases, and multi-layer data such as JSON and XML, and thus can perform mixed computations.\n\nWith the open system, data scientists can process data coming from various sources directly and efficiently, getting rid of the time spent in data organization and data import/export and thus increasing data processing efficiency.\nHigh portability and enterprise-friendly\nSPL also offers the proprietary file format that is high-performance and portable.\n\nIn contrast, Python lacks a proprietary storage solution. Text files are slow, and databases result in the loss of portability.\nSPL is enterprise-friendly because it is developed in pure Java. After data exploration and analysis, data scientists can integrate SPL with the application by embedding its jars into the latter, facilitating smooth transition from outside to inside.\nWith the convenient to use, easy to debug, high-performance and integration-friendly SPL, data scientists can be freed from the heavy coding work and devote more time to their own businesses.\nesProc SPL is an open-source tool, and its source code is available here.",
    "category": "å…¶ä»–"
  },
  {
    "title": "How to Network Online: Naturally Find Clients, Develop Partnerships And Make Smart Friends",
    "content": "https://benjaminsnotes.substack.com/p/how-to-network",
    "url": "https://www.indiehackers.com/post/how-to-network-online-naturally-find-clients-develop-partnerships-and-make-smart-friends-d70b49264c",
    "crawl_time": "2025-02-10 06:42:38",
    "translated_content": "https://benjaminsnotes.substack.com/p/how-to-network",
    "category": "å…¶ä»–"
  }
]